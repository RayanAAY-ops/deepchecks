{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Deepchecks for Recommender System\n",
    "===============================================================================\n",
    "\n",
    "Deepchecks Recommender  is your go-to tool for developing and evaluating recommender system models, ensuring their robustness before deployment. Our comprehensive testing package not only detects potential failures but also saves you valuable development time. In this quickstart guide, you'll learn how to utilize Deepchecks Recommender to analyze and evaluate various aspects of your recommender system, including data quality, leakage, product associations, cold start detection, and drift. Let's get started.\n",
    "\n",
    "**Step 1: Data Preparation and Auto Analysis**\n",
    "---------------------------------------------\n",
    "\n",
    "To run Deepchecks Recommender, make sure you have the following data for both your training and testing sets:\n",
    "\n",
    "1. User-Item Interaction Data: A structured dataset containing information about user-item interactions. Each record represents a user's interaction with an item, such as viewing, purchasing, or rating.\n",
    "\n",
    "2. Product Information: Additional information about the items in your catalog, like product categories, descriptions, or features.\n",
    "\n",
    "3. User Information (Optional): If available, user-specific data such as demographics, preferences, or historical behavior can enhance the evaluation.\n",
    "\n",
    "4. Your labels : These are not needed for checks that don't require labels (such as the Cold Start Detection check or most data integrity checks), but are needed for many other checks.\n",
    "\n",
    "5. Your model's predictions: These are needed only for the model related checks, shown in the Model Evaluation section of this guide."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create train and validation split.\n",
    "def split_train_validation(interaction_df : pd.DataFrame,\n",
    "                           session_col : str,\n",
    "                           item_col : str,\n",
    "                           timestamp_col : str, \n",
    "                           test_percentage=0.2,\n",
    "                           random_seed=None):\n",
    "    \n",
    "    assert set([session_col, item_col, timestamp_col]).issubset(interaction_df.columns)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    interaction_df = interaction_df.sort_values(timestamp_col,ascending=True)\n",
    "    split_index = int(len(interaction_df) * (1 - test_percentage))\n",
    "\n",
    "    train = interaction_df.iloc[:split_index]\n",
    "    test = interaction_df.iloc[split_index:]\n",
    "\n",
    "    # Let's discard overlapping sessions to make train and valid sets disjoints and independent.\n",
    "    overlapping_sessions = set(train[session_col]).intersection(set(test[session_col]))\n",
    "\n",
    "    test = test[~test[session_col].isin(overlapping_sessions)]\n",
    "\n",
    "    data_to_calculate_validation_score = []\n",
    "    new_test = []\n",
    "    for grp in test.groupby(session_col):\n",
    "        cutoff = np.random.randint(1, grp[1].shape[0]) # we want at least a single item in our validation data for each userId\n",
    "        new_test.append(grp[1].iloc[:cutoff])\n",
    "        data_to_calculate_validation_score.append(grp[1].iloc[cutoff:])\n",
    "\n",
    "    test = pd.concat(new_test).reset_index(drop=True)\n",
    "    \n",
    "    test_labels = pd.concat(data_to_calculate_validation_score).reset_index(drop=True)\n",
    "    assert test[timestamp_col].max() < test_labels[timestamp_col].max()\n",
    "    \n",
    "    test_labels = test_labels.groupby(session_col)[item_col].apply(list)\n",
    "    assert (test[session_col].unique() == test_labels.index.values).sum()\n",
    "    \n",
    "    return train,test,test_labels\n",
    "\n",
    "# Create user features.\n",
    "def user_features(X_interaction : pd.DataFrame):\n",
    "    user_df = pd.DataFrame()\n",
    "    user_df['userId'] = X_interaction['userId'].unique()\n",
    "    user_df['mean_rating'] = X_interaction.groupby(\"userId\")['rating'].mean().values\n",
    "    user_df['median_rating'] = X_interaction.groupby(\"userId\")['rating'].median().values\n",
    "    user_df['std_rating'] = X_interaction.groupby(\"userId\")['rating'].std().values\n",
    "    user_df['session_length'] = X_interaction.groupby(\"userId\")['rating'].count().values\n",
    "\n",
    "    user_df['min_rating'] = X_interaction.groupby(\"userId\")['rating'].min().values\n",
    "    user_df['max_rating'] = X_interaction.groupby(\"userId\")['rating'].max().values\n",
    "    user_df['last_timestamp'] = X_interaction.groupby(\"userId\")['timestamp'].tail(1).values\n",
    "    user_df['last_timestamp'] = user_df['last_timestamp'].apply(lambda x: pd.to_datetime(x).timestamp()).astype(int)\n",
    "\n",
    "    user_df['sum_rating'] = X_interaction.groupby(\"userId\")['rating'].sum().values\n",
    "\n",
    "\n",
    "    user_df['noise'] = np.random.normal(0,1,size=(user_df['session_length'].shape))\n",
    "\n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this guide, we\\'ll use a small subset of the [movieLens](https://grouplens.org/datasets/movielens/) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  955k  100  955k    0     0   938k      0  0:00:01  0:00:01 --:--:--  942k\n",
      "Archive:  ml-latest-small.zip\n"
     ]
    }
   ],
   "source": [
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!unzip -n ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1e+03 ns, total: 3 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "# Load interaction data.\n",
    "df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "\n",
    "# Load item data.\n",
    "movie_df =  pd.read_csv(\"ml-latest-small/movies.csv\")\n",
    "\n",
    "# Split interaction data into train and validation data.\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x : datetime.fromtimestamp(x))\n",
    "X_train_interactions, X_test_interactions, y_test = split_train_validation(interaction_df=df,\n",
    "                                           session_col='userId',\n",
    "                                           item_col ='movieId',\n",
    "                                           timestamp_col='timestamp',\n",
    "                                           test_percentage=0.6)\n",
    "\n",
    "# Create User Dataframe\n",
    "train_users_df = user_features(X_train_interactions)\n",
    "valid_users_df = user_features(X_test_interactions)\n",
    "\n",
    "# Add targets to the valid users\n",
    "valid_users_df = pd.merge(valid_users_df,y_test.rename('target'),how=\"left\",on=\"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a Recommender Datasets\n",
    "================================\n",
    "\n",
    "We can now create a Dataset object for the train and test dataframes. This object is\n",
    "used to pass your data to the deepchecks checks.\n",
    "\n",
    "To create a Recommender Dataset, the only required argument is the data\n",
    "itself, but passing only the data will prevent multiple checks from\n",
    "running. In this example we\\'ll define the task type\n",
    "and finally define the\n",
    "metadata columns (the other columns in the dataframe) which we\\'ll use later on in the\n",
    "guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 1 categorical features were inferred.: max_rating\n",
      "deepchecks - WARNING - It is recommended to initialize Dataset with categorical features by doing \"Dataset(df, cat_features=categorical_list)\". No categorical features were passed, therefore heuristically inferring categorical features in the data. 0 categorical features were inferred.\n"
     ]
    }
   ],
   "source": [
    "from deepchecks.recommender import InteractionDataset, UserDataset,ItemDataset\n",
    "\n",
    "# Interaction Datasets\n",
    "#################################################################\n",
    "train_interaction_ds = InteractionDataset(df=X_train_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "valid_interaction_ds = InteractionDataset(df=X_test_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "# User Datasets\n",
    "#################################################################\n",
    "train_user_ds = UserDataset(df = train_users_df,\n",
    "                label = None,                    \n",
    "                features=['mean_rating', 'session_length','median_rating','std_rating','noise','min_rating','max_rating','last_timestamp','sum_rating'],\n",
    "                cat_features=None)\n",
    "\n",
    "valid_user_ds = UserDataset(df = valid_users_df,\n",
    "                label = \"target\",                    \n",
    "                features=['mean_rating', 'session_length','median_rating','std_rating','noise','min_rating','max_rating','last_timestamp','sum_rating'],\n",
    "                cat_features=None)\n",
    "            \n",
    "# Item Dataset\n",
    "#################################################################\n",
    "item_ds = ItemDataset(df=movie_df,\n",
    "                      item_column_name='title',\n",
    "                      features=['title','genres'],\n",
    "                      cat_features=['title','genres'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recommender Model Class\n",
    "================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially used a covisitation recommender algorithm to generate a diverse set of candidate items for recommendation. This approach leverages the co-occurrence patterns of items in user sessions to suggest a pool of potential items that might be of interest to users.\n",
    "\n",
    "\n",
    "The co-occurrence recommender leverages the patterns of item co-occurrence in user sessions to generate relevant recommendations. It's based on the idea that if two items frequently appear together in user sessions, they might be related or have some inherent similarity that can be exploited for recommendations.\n",
    "\n",
    "\n",
    "1. **Fitting**: Using training data, we track associations between previous and current items based on their co-occurrence in user sessions.\n",
    "\n",
    "2. **Predicting**: For each user in validation data, we accumulate associated item counts from their session items.\n",
    "\n",
    "3. **Recommendations**: We suggest the most common associated items, excluding those already in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOccurrenceRecommender:\n",
    "    def __init__(self, user_col, item_col, num_predictions=20):\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.num_predictions = num_predictions\n",
    "        self.co_occurences = defaultdict(Counter)\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        # Make a copy of the training data\n",
    "        filtered_interactions = X_train.copy()\n",
    "        \n",
    "        # Create a new column with the previous item in each user session\n",
    "        prev_item_col = f'prev_{self.item_col}'\n",
    "        filtered_interactions[prev_item_col] = filtered_interactions.groupby(self.user_col)[self.item_col].shift(1).astype(\"Int64\").dropna()\n",
    "        \n",
    "        # Create a DataFrame with columns 'previous item' and 'item'\n",
    "        products_association_df = filtered_interactions[[prev_item_col, self.item_col]].copy().dropna()\n",
    "        \n",
    "        # Generate associations between 'previous item' and 'item'\n",
    "        for row in products_association_df.itertuples(index=False):\n",
    "            self.co_occurences[row[0]][row[1]] += 1\n",
    "\n",
    "    def predict(self, X_valid):\n",
    "        user_recommendations = {}\n",
    "\n",
    "        X_test_session_items = X_valid.groupby(self.user_col)[self.item_col].apply(list)\n",
    "\n",
    "        for user, items in X_test_session_items.items():\n",
    "            items = list(dict.fromkeys(items[::-1]))\n",
    "\n",
    "            counter = Counter()\n",
    "\n",
    "            for item in items:\n",
    "                subsequent_item_counter = self.co_occurences.get(item)\n",
    "                if subsequent_item_counter:\n",
    "                    counter += subsequent_item_counter\n",
    "            \n",
    "            # Get the top N recommended items based on the associations\n",
    "            recommendations = [item for item, cnt in counter.most_common(self.num_predictions) if item not in items]\n",
    "            user_recommendations[user] = recommendations\n",
    "        \n",
    "        recommendations_series = pd.Series(user_recommendations)\n",
    "        return recommendations_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, this method uses item co-occurrence patterns to provide recommendations by suggesting items that often appear together in user sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 2.86 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "recommender = CoOccurrenceRecommender(user_col='userId',\n",
    "                                      item_col='movieId',\n",
    "                                      num_predictions=50)\n",
    "\n",
    "# Fit the model on the training data\n",
    "recommender.fit(X_train_interactions)\n",
    "\n",
    "# Generate predictions for the validation data\n",
    "predictions = recommender.predict(X_test_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayanaay/Downloads/testing_deepchecks/deepchecks/deepchecks/recommender/ranking.py:181: UserWarning:\n",
      "\n",
      "Reciprocal rank is a reranking metric; missing relevant items may impact results.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db8bc636eb044e7b9e5547f94a1619b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Sample Performance</b></h4>'), HTML(value='<p>Summarize given recommender sy…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks import SamplePerformance\n",
    "\n",
    "perf_check = SamplePerformance(scorers=[\n",
    "                                   'mean_average_recall_at_k',\n",
    "                                   'mean_average_precision_at_k',\n",
    "                                   'mean_average_f1_at_k',\n",
    "                                   'mean_average_ndcg_k',\n",
    "                                   'mean_reciprocal_rank'\n",
    "                                   ],\n",
    "                                   k = 20\n",
    "                                )\n",
    "\n",
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=predictions.values.tolist())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayanaay/Downloads/testing_deepchecks/deepchecks/deepchecks/recommender/ranking.py:181: UserWarning:\n",
      "\n",
      "Reciprocal rank is a reranking metric; missing relevant items may impact results.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69b90129e5b34e7e8d04420ba32d17ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Sample Performance</b></h4>'), HTML(value='<p>Summarize given recommender sy…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "perf_check.add_condition_greater_than(threshold=0.1, class_mode='all')\n",
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=predictions.values.tolist())\n",
    "result.show(show_additional_outputs=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DateTrainTestLeakageOverlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------\n",
    "# Copyright (C) 2021-2023 Deepchecks (https://www.deepchecks.com)\n",
    "#\n",
    "# This file is part of Deepchecks.\n",
    "# Deepchecks is distributed under the terms of the GNU Affero General\n",
    "# Public License (version 3 or later).\n",
    "# You should have received a copy of the GNU Affero General Public License\n",
    "# along with Deepchecks.  If not, see <http://www.gnu.org/licenses/>.\n",
    "# ----------------------------------------------------------------------------\n",
    "#\n",
    "\"\"\"Module of DateTrainTestLeakageOverlap check.\"\"\"\n",
    "from deepchecks.core import CheckResult\n",
    "from deepchecks.tabular import TrainTestCheck\n",
    "from deepchecks.recommender import Context\n",
    "from deepchecks.utils.strings import format_datetime, format_percent\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "__all__ = ['DateTrainTestLeakageOverlap']\n",
    "\n",
    "\n",
    "class DateTrainTestLeakageOverlap(TrainTestCheck):\n",
    "    \"\"\"Ensure there's no overlap between training and testing data based on dates.\n",
    "\n",
    "    Time validation (validation_per_user=True) is used to validate in the last time window.\n",
    "    For validation per user (validation_per_user=True), use \"N leave out\" validation schema.\n",
    "    It involves withholding the most recent n interactions for each user for validation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    validation_per_user : bool, default: True\n",
    "         if set to True :evaluate the performance of the model by leaving out N items for each user.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        validation_per_user: bool = False,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.validation_per_user = validation_per_user\n",
    "\n",
    "    def run_logic(self, context: Context) -> CheckResult:\n",
    "        \"\"\"Run check.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        CheckResult\n",
    "            value is the ratio of date leakage.\n",
    "            data is html display of the checks' textual result.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        DeepchecksValueError\n",
    "            If one of the datasets is not a Dataset instance with an date\n",
    "        \"\"\"\n",
    "        train_date = context.train.datetime_col\n",
    "        test_date = context.test.datetime_col\n",
    "        display = []\n",
    "        if self.validation_per_user is False:\n",
    "            _, max_train_date = min(train_date), max(train_date)\n",
    "            min_test_date, max_test_date = min(test_date), max(test_date)\n",
    "\n",
    "            dates_leaked = sum(date < max_train_date for date in test_date)\n",
    "            if dates_leaked > 0:\n",
    "                leakage_ratio = dates_leaked / context.test.n_samples\n",
    "            else:\n",
    "                leakage_ratio = 0\n",
    "\n",
    "            # Optimize the plotting by sampling data\n",
    "            sample_size = min(len(train_date), len(test_date), 100_000)\n",
    "            train_sample = np.random.choice(train_date, sample_size, replace=False)\n",
    "            test_sample = np.random.choice(test_date, sample_size, replace=False)\n",
    "            # Ensure min and max dates of each set are included in the sample\n",
    "            train_sample = np.append(train_sample,\n",
    "                                     pd.to_datetime([min(train_date), max(train_date)]))\n",
    "            test_sample = np.append(test_sample,\n",
    "                                    pd.to_datetime([min(test_date), max(test_date)]))\n",
    "\n",
    "            fig = go.Figure()\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=pd.to_datetime(sorted(train_sample.tolist())),\n",
    "                y=[1] * len(train_sample),\n",
    "                mode='lines',\n",
    "                name='Train timestamps',\n",
    "                line=dict(color='blue')\n",
    "            ))\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=pd.to_datetime(sorted(test_sample.tolist())),\n",
    "                y=[2] * len(test_sample),\n",
    "                mode='lines',\n",
    "                name='Validation timestamps',\n",
    "                line=dict(color='red')\n",
    "            ))\n",
    "            # Add Markers for max_train_date and min_test_date\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[max(train_date), max(train_date)], y=[0, 1],\n",
    "                name='Max Train Timestamp',\n",
    "                line=dict(color='blue', width=2, dash='dot'))\n",
    "            )\n",
    "\n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=[min(test_date), min(test_date)], y=[3, 2],\n",
    "                name='Min Test Timestamp',\n",
    "                line=dict(color='red', width=2, dash='dot'))\n",
    "            )\n",
    "\n",
    "            fig.update_layout(\n",
    "                title='Temporal Comparison of train and validation timestamps.',\n",
    "                yaxis=dict(\n",
    "                    range=[0, 3],\n",
    "                    tickvals=[1, 2],\n",
    "                    ticktext=['Train', 'Validation']\n",
    "                ),\n",
    "                xaxis=dict(\n",
    "                    title='Timestamps',\n",
    "                    type='date',  # Set the x-axis type to 'date' for proper date formatting\n",
    "                    tickformat='%Y-%m-%d',  # Customize the date format as needed\n",
    "                ),\n",
    "                xaxis_title='Timestamps',\n",
    "                yaxis_title='Set',\n",
    "                height=400\n",
    "            )\n",
    "\n",
    "            text = f'{format_percent(leakage_ratio)} of test data samples are in the date range '\\\n",
    "                f'{format_datetime(min_test_date)} - {format_datetime(max_test_date)}'\\\n",
    "                f', which occurs before last training data date ({format_datetime(max_train_date)})'\n",
    "            display.append(text)\n",
    "            return_value={'max_train_date': max_train_date,\n",
    "                          'min_test_date': min_test_date\n",
    "                                }\n",
    "            display.append(fig)\n",
    "        else:\n",
    "            user_id =context.train.user_index_name\n",
    "            train_grouped = context.train.data.groupby(user_id)\n",
    "            test_grouped = context.test.data.groupby(user_id)\n",
    "            leakage_ratios = []\n",
    "            \n",
    "            for user_id, train_group in train_grouped:\n",
    "                if user_id in list(test_grouped.groups.keys()):\n",
    "                    \n",
    "                    train_date = train_group['timestamp']\n",
    "                    _, max_train_date = min(train_date), max(train_date)\n",
    "\n",
    "                    test_group = test_grouped.get_group(user_id)\n",
    "                    test_date = test_group['timestamp']\n",
    " \n",
    "                    dates_leaked = sum(date < max_train_date for date in test_date)\n",
    "                    if dates_leaked > 0:\n",
    "                        leakage_ratio = dates_leaked / len(test_group)\n",
    "                        leakage_ratios.append(leakage_ratio)\n",
    "                else:\n",
    "                    continue\n",
    "            if len(leakage_ratios) > 0:\n",
    "                average_leakage_ratio = sum(leakage_ratios) / len(leakage_ratios)\n",
    "                text = f'There is an average leak of {format_percent(average_leakage_ratio)} per user. In other words, {format_percent(average_leakage_ratio)} of the  validation set of a user, appears before the maximum timestamp of the training set.'\n",
    "                display.append(text)\n",
    "                \n",
    "                fig = go.Figure()\n",
    "                fig.add_trace(go.Bar(x=['Leakage Percentage'], y=[100*average_leakage_ratio],width=0.3))\n",
    "\n",
    "                fig.update_layout(\n",
    "                    title_text=\"Leaky User Overview\",\n",
    "                    yaxis_title=\"Proportion (%)\"\n",
    "                    )\n",
    "\n",
    "                display.append(fig)\n",
    "            else:\n",
    "                average_leakage_ratio = 0   \n",
    "                text = \"No leak between users.\"\n",
    "                display.append(text)\n",
    "\n",
    "            # Display the figure\n",
    "            return_value={'average_leakage_ratio' : average_leakage_ratio}  \n",
    "\n",
    "        return CheckResult(value=return_value,\n",
    "                           header='Date Train-Test Leakage (overlap)',\n",
    "                           display=display)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83c55b9875d477392bd1068af39dae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Date Train-Test Leakage (overlap)</b></h4>'), HTML(value='<p>Ensure there\\'s…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#from deepchecks.recommender.checks import DateTrainTestLeakageOverlap\n",
    "\n",
    "check = DateTrainTestLeakageOverlap(validation_per_user=False)\n",
    "\n",
    "result = check.run(train_dataset=train_interaction_ds,\n",
    "                   test_dataset=valid_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold Start Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayanaay/Downloads/testing_deepchecks/deepchecks/deepchecks/tabular/dataset.py:236: UserWarning:\n",
      "\n",
      "Dataframe index has duplicate indexes, setting index to [0,1..,n-1].\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf21906ccfa448c9b91368d87c180762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Cold Start Detection</b></h4>'), HTML(value='<p>Retrieve cold start users an…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks  import ColdStartDetection\n",
    "\n",
    "all_interaction_ds = train_interaction_ds + valid_interaction_ds\n",
    "check = ColdStartDetection()\n",
    "result = check.run(all_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product Association\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayanaay/Downloads/testing_deepchecks/deepchecks/venv/lib/python3.9/site-packages/pandas/core/dtypes/cast.py:1201: RuntimeWarning:\n",
      "\n",
      "overflow encountered in cast\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b610f6f4c384de88a75aaabbab54776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Product Association</b></h4>'), HTML(value='<p>    Check for performing non-…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks import ProductAssociation\n",
    "\n",
    "\n",
    "check = ProductAssociation(max_timestamp_delta=3600)\n",
    "result = check.run(all_interaction_ds,\n",
    "                   item_dataset=item_ds\n",
    "                   )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Session Length Drift\n",
    "===================================\n",
    "\n",
    "Also in the \\\"Didn\\'t Pass\\\" tab we can see the two segment performance\n",
    "checks - Property Segment Performance and Metadata Segment Performance.\n",
    "These use the metadata columns  of user related information OR our\n",
    "calculated properties to try and **automatically** detect significant data\n",
    "segments on which our model performs badly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dea2bafde2422c82d6f46ca6e314d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Session Length drift</b></h4>'), HTML(value='<p>Check for user session lengt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks import UserSessionDrift\n",
    "\n",
    "check = UserSessionDrift()\n",
    "\n",
    "result = check.run(train_dataset = train_interaction_ds,\n",
    "                   test_dataset = valid_interaction_ds)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Prediction Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac1164b58e574f2cb8be061939931e86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Prediction Popularity Drift</b></h4>'), HTML(value='<p>Compute popularity dr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks import PredictionPopularityDrift\n",
    "\n",
    "check = PredictionPopularityDrift()\n",
    "result = check.run(valid_user_ds,\n",
    "                   y_pred=predictions.values.tolist(),\n",
    "                   interaction_dataset=train_interaction_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Label Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb231e3fc91c4a80946443fdd00b13d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Label Popularity Drift</b></h4>'), HTML(value='<p>Compute popularity drift b…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks import LabelPopularityDrift\n",
    "\n",
    "check = LabelPopularityDrift()\n",
    "result = check.run(valid_user_ds,\n",
    "                   interaction_dataset=train_interaction_ds+valid_interaction_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b731c9a07c1d480c99e3912e6970e1c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Segment Performance</b></h4>'), HTML(value='<p>Display performance score seg…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks import SegmentPerformance\n",
    "import traceback\n",
    "try:\n",
    "    result = SegmentPerformance(feature_1='session_length',\n",
    "                       feature_2='mean_rating',\n",
    "                       alternative_scorer={'recall':'mean_average_precision_at_k'},\n",
    "                       max_segments=3\n",
    "                       ).run(valid_user_ds, y_pred=predictions.values.tolist())\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeakSegmentPerformance\n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74eb3843336e459aa673f1c11efba729",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Weak Segments Performance</b></h4>'), HTML(value='<p>Search for segments wit…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from deepchecks.recommender.checks import WeakSegmentsPerformance\n",
    "\n",
    "check = WeakSegmentsPerformance(columns=['mean_rating',\n",
    "                                        'session_length',\n",
    "                                        'median_rating',\n",
    "                                        'std_rating',\n",
    "                                        'noise',\n",
    "                                        'min_rating',\n",
    "                                        'max_rating',\n",
    "                                        'last_timestamp',\n",
    "                                        'sum_rating'],\n",
    "                                alternative_scorer={'mean_average_recall_at_k': 'mean_average_recall_at_k'},\n",
    "                                segment_minimum_size_ratio=0.1,\n",
    "                                categorical_aggregation_threshold=0.5)\n",
    "check.add_condition_segments_relative_performance_greater_than(0.1)\n",
    "\n",
    "result = check.run(valid_user_ds, y_pred=predictions.values.tolist())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate more candidates : a Word2Vec Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration of the co-occurrence recommender was just one facet of our strategy. In tandem, let's incorporate the Word2Vec model, which delves deeper into the semantic relationships between items. By mapping items into a multi-dimensional vector space, the Word2Vec model identifies underlying similarities between items, even when co-occurrence patterns are not explicit.\n",
    "\n",
    "\n",
    "\n",
    "To fit the Word2Vec model:\n",
    "\n",
    "1. **Data Preparation**: We combine training and validation data (without labels), grouping user interactions by the user column.\n",
    "\n",
    "2. **Creating Sentences**: User interactions become \"sentences\" for the Word2Vec model.\n",
    "\n",
    "3. **Training Word2Vec**: The model is trained using collected sentences.\n",
    "\n",
    "4. **Index Mapping**: We map item IDs to their Word2Vec indices.\n",
    "\n",
    "5. **Building K-NN Graph**: here we use ``annoy`` to build the knn-graph, which is suitable for large dataset, avoiding memory issues.\n",
    "\n",
    "6. **Adding Items to Graph**: For each item, we add its index and vector to the k-nearest neighbor graph.\n",
    "\n",
    "7. **Building K-NN Structure**: The graph is built for k-nearest neighbor queries.\n",
    "\n",
    "8. **Prediction**:  the recommendations will be the k-nearest item of the last item each user interacted with, using the knn-graph based on items embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "class Word2VecRecommender:\n",
    "    def __init__(self, user_col, item_col, vector_size=8, num_recommendations=20):\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.vector_size = vector_size\n",
    "        self.num_recommendations = num_recommendations\n",
    "        self.w2vec = None\n",
    "        self.knn_graph = None\n",
    "        self.item2idx = None\n",
    "    \n",
    "    def fit(self, X_train, X_valid):\n",
    "        sentences_df = pd.concat([X_train, X_valid]).groupby(self.user_col)[self.item_col].apply(list).reset_index()\n",
    "        sentences_df.rename(columns={self.item_col: 'sentence'}, inplace=True)\n",
    "\n",
    "        sentences = sentences_df['sentence'].to_list()\n",
    "\n",
    "        self.w2vec = Word2Vec(sentences=sentences, vector_size=self.vector_size, min_count=1)\n",
    "\n",
    "        self.item2idx = {aid: i for i, aid in enumerate(self.w2vec.wv.index_to_key)}\n",
    "\n",
    "        self.knn_graph = AnnoyIndex(self.vector_size, 'angular')\n",
    "\n",
    "        for aid, idx in self.item2idx.items():\n",
    "            self.knn_graph.add_item(idx, self.w2vec.wv.vectors[idx])\n",
    "\n",
    "        self.knn_graph.build(30)\n",
    "    \n",
    "    def predict(self, X_valid):\n",
    "        user_recommendations = {}\n",
    "\n",
    "        X_test_session_items = X_valid.groupby(self.user_col)[self.item_col].apply(list)\n",
    "\n",
    "        for user, items in X_test_session_items.items():\n",
    "            items = list(dict.fromkeys(items[::-1]))\n",
    "\n",
    "            most_recent_aid = items[0]\n",
    "\n",
    "            nns = [self.w2vec.wv.index_to_key[i] for i in \n",
    "                   self.knn_graph.get_nns_by_item(self.item2idx[most_recent_aid], self.num_recommendations + 1)[1:]]\n",
    "\n",
    "            recommendations = [item for item in nns if item not in items]\n",
    "            user_recommendations[user] = recommendations\n",
    "\n",
    "        word2vec_recommendations = pd.Series(user_recommendations)\n",
    "        return word2vec_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 1e+03 ns, total: 2 µs\n",
      "Wall time: 3.1 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "word2vec_recommender = Word2VecRecommender(user_col='userId',\n",
    "                                           item_col='movieId',\n",
    "                                           vector_size=8,\n",
    "                                           num_recommendations=50)\n",
    "word2vec_recommender.fit(X_train_interactions, X_test_interactions)\n",
    "word2vec_predictions = word2vec_recommender.predict(X_test_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rayanaay/Downloads/testing_deepchecks/deepchecks/deepchecks/recommender/ranking.py:181: UserWarning:\n",
      "\n",
      "Reciprocal rank is a reranking metric; missing relevant items may impact results.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "719e991d0c1a429094c31785a4e24f0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h4><b>Sample Performance</b></h4>'), HTML(value='<p>Summarize given recommender sy…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Word2Vec Performance\n",
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=word2vec_predictions.values.tolist())\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine predictions/candidates\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = pd.concat([predictions.rename(\"co-occurence_pred\"),word2vec_predictions.rename(\"word2vec_pred\")],axis=1)\n",
    "all_predictions['all_preds'] = all_predictions['co-occurence_pred'] + all_predictions['word2vec_pred']\n",
    "all_predictions['all_preds'] = all_predictions['all_preds'].apply(lambda x : list(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking : a LightGBMRanker Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating a large set of candidate items, let's use the LGBMRanker algorithm for reranking. \n",
    "\n",
    "LGBMRanker is a gradient boosting algorithm designed for ranking tasks, and it's well-suited for reranking a list of items based on their predicted relevance to users.\n",
    "\n",
    "- The reranking process involves considering multiple features or signals associated with items and users to estimate the relevance of items for individual users. The algorithm takes into account various factors such as item popularity, user behavior, and more.\n",
    "\n",
    "- By reranking the candidates using LGBMRanker, we will reorder the candidate items for each user in a way that aims to improve the overall relevance of the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess item features\n",
    "item_popularity = pd.concat([X_train_interactions,X_test_interactions],axis=0)['movieId'].value_counts().to_dict()\n",
    "\n",
    "def preprocess_item_features(movie_df, item_popularity):\n",
    "    movie_df[['genre_1', 'genre_2', 'genre_3']] = movie_df['genres'].str.split('|', expand=True).iloc[:, :3]\n",
    "    popularity_df = pd.DataFrame(list(item_popularity.items()), columns=['movieId', 'popularity'])\n",
    "    item_features = movie_df.merge(popularity_df, on='movieId').drop(['title', 'genres'], axis=1)\n",
    "    return item_features\n",
    "\n",
    "item_features = preprocess_item_features(movie_df, item_popularity)\n",
    "\n",
    "# Preprocess user features\n",
    "def preprocess_user_features(valid_users_df):\n",
    "    user_features = valid_users_df.copy().drop('target', axis=1)\n",
    "    return user_features\n",
    "\n",
    "user_features = preprocess_user_features(valid_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode candidates and true labels\n",
    "def explode_candidates_true_labels(predictions, y_test, item_features, user_features):\n",
    "    candidates = predictions.reset_index()\n",
    "    candidates.columns = ['userId', 'item']\n",
    "    candidates = candidates.explode('item')\n",
    "\n",
    "    df = candidates.merge(item_features, left_on='item', right_on='movieId', right_index=True, how='left').fillna(-1)\n",
    "    df = df.merge(user_features, on='userId', how='left').fillna(-1)\n",
    "\n",
    "    true_labels = y_test.reset_index()\n",
    "    true_labels.columns = ['userId', 'item']\n",
    "    true_labels = true_labels.explode('item')\n",
    "    true_labels['gt'] = 1\n",
    "\n",
    "    df_ = pd.merge(df, true_labels, on=['userId', 'item'], how='left').fillna(0)\n",
    "    df_['gt'] = df_['gt'].astype(int)\n",
    "    \n",
    "    object_columns = df_.select_dtypes(include=['object']).columns\n",
    "    df_[object_columns] = df_[object_columns].astype('category')\n",
    "\n",
    "    return df_\n",
    "\n",
    "df_ = explode_candidates_true_labels(all_predictions['all_preds'], y_test, item_features, user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001157 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1372\n",
      "[LightGBM] [Info] Number of data points in the train set: 18953, number of used features: 13\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Best NDCG Score: OrderedDict([('ndcg@28', 0.46363168172880154), ('ndcg@42', 0.5041791603312679), ('ndcg@84', 0.5830413678596422)])\n"
     ]
    }
   ],
   "source": [
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "# Splitting the data into train and validation\n",
    "def split_data(df_):\n",
    "    unique_users = df_['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "    train_size = int(len(unique_users) * 0.7)\n",
    "    train_users = unique_users[:train_size]\n",
    "    valid_users = unique_users[train_size:]\n",
    "    train_df = df_[df_['userId'].isin(train_users)]\n",
    "    valid_df = df_[df_['userId'].isin(valid_users)]\n",
    "\n",
    "    d_train = train_df.groupby(\"userId\").size().values.tolist()\n",
    "    d_valid = valid_df.groupby(\"userId\").size().values.tolist()\n",
    "\n",
    "    return train_df, valid_df, d_train, d_valid\n",
    "\n",
    "train_df, valid_df, d_train, d_valid = split_data(df_)\n",
    "\n",
    "\n",
    "# Training and evaluation\n",
    "def train_evaluate_lgbm_ranker(train_df, valid_df, features_col, target_col, d_train, d_valid):\n",
    "    ranker = LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "    )\n",
    "    n_eval = int(df_.groupby(\"userId\").size().mean())\n",
    "    ranker.fit(\n",
    "        train_df[features_col],\n",
    "        train_df[target_col],\n",
    "        group=d_train,\n",
    "        eval_set=[(valid_df[features_col], valid_df[target_col])],\n",
    "        eval_group=[d_valid],\n",
    "        eval_metric=\"ndcg\",\n",
    "        eval_at=[int(n_eval/3), int(n_eval/2),n_eval]\n",
    "    )\n",
    "\n",
    "    return ranker, ranker.best_score_['valid_0']\n",
    "\n",
    "features_col = ['genre_1', 'genre_2', 'genre_3',\n",
    "       'popularity', 'mean_rating', 'median_rating', 'std_rating',\n",
    "       'session_length', 'min_rating', 'max_rating', 'last_timestamp',\n",
    "       'sum_rating', 'noise']\n",
    "target_col = ['gt']\n",
    "\n",
    "ranker, best_ndcg_score = train_evaluate_lgbm_ranker(train_df, valid_df, features_col, target_col, d_train, d_valid)\n",
    "print(f\"Best NDCG Score: {best_ndcg_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6d/kc00__6922zf13qqx83smlfm0000gn/T/ipykernel_2738/1576030944.py:5: SettingWithCopyWarning:\n",
      "\n",
      "\n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generating predictions\n",
    "def generate_predictions(ranker, candidates, features_col):\n",
    "    scores = ranker.predict(candidates[features_col])\n",
    "\n",
    "    candidates['score'] = scores\n",
    "    predictions_lgbm = (\n",
    "        candidates.sort_values(by=['userId', 'score'], ascending=[True, False])\n",
    "        .groupby('userId')\n",
    "        .apply(lambda group: group['item'].head(20).tolist())\n",
    "    )\n",
    "    return predictions_lgbm\n",
    "\n",
    "predictions_lgbm = generate_predictions(ranker, valid_df, features_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_valid2 = pd.merge(predictions_lgbm.rename(\"lgbm_pred\"),y_test.rename(\"true_labels\"),how=\"inner\",on=\"userId\")['true_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean average recall 0.13333333333333333\n",
      "mean average precision 0.06609581679894179\n",
      "mean average f1 0.0883800143293696\n"
     ]
    }
   ],
   "source": [
    "from deepchecks.recommender.ranking import mean_average_recall_at_k, mean_average_precision_at_k, mean_average_f1_at_k\n",
    "\n",
    "print(\"mean average recall\",mean_average_recall_at_k(y_true_valid2.values.tolist(),predictions_lgbm.values.tolist()))\n",
    "print(\"mean average precision\",mean_average_precision_at_k(y_true_valid2.values.tolist(),predictions_lgbm.values.tolist()))\n",
    "print(\"mean average f1\",mean_average_f1_at_k(y_true_valid2.values.tolist(),predictions_lgbm.values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the results, using a reranker algorithm like LGBMRanker enhances the performance of our recommender system.\n",
    "\n",
    "\n",
    "In summary, we've combined the strengths of both the covisitation recommender, which captures item co-occurrence patterns, and the Word2Vec model, which captures item semantics, to generate an extensive list of candidate items. Then, by using the LGBMRanker algorithm to rerank these candidates, we've achieved better recommendations by considering multiple factors related to item-user interactions and relevance. This approach reflects a well-rounded and effective strategy for improving the recommendation quality of our system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
