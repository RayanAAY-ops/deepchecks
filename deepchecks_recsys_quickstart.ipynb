{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Deepchecks for Recommender System\n",
    "===============================================================================\n",
    "\n",
    "Deepchecks Recommender  is your go-to tool for developing and evaluating recommender system models, ensuring their robustness before deployment. Our comprehensive testing package not only detects potential failures but also saves you valuable development time. In this quickstart guide, you'll learn how to utilize Deepchecks Recommender to analyze and evaluate various aspects of your recommender system, including data quality, leakage, product associations, cold start detection, and drift. Let's get started.\n",
    "\n",
    "**Step 1: Data Preparation and Auto Analysis**\n",
    "---------------------------------------------\n",
    "\n",
    "To run Deepchecks Recommender, make sure you have the following data for both your training and testing sets:\n",
    "\n",
    "1. User-Item Interaction Data: A structured dataset containing information about user-item interactions. Each record represents a user's interaction with an item, such as viewing, purchasing, or rating.\n",
    "\n",
    "2. Product Information: Additional information about the items in your catalog, like product categories, descriptions, or features.\n",
    "\n",
    "3. User Information (Optional): If available, user-specific data such as demographics, preferences, or historical behavior can enhance the evaluation.\n",
    "\n",
    "4. Your labels : These are not needed for checks that don't require labels (such as the Cold Start Detection check or most data integrity checks), but are needed for many other checks.\n",
    "\n",
    "5. Your model's predictions: These are needed only for the model related checks, shown in the Model Evaluation section of this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has been done\n",
    "===============\n",
    "\n",
    "- pass Pylint for all the checks ( docstring, snake_case naming ...)\n",
    "\n",
    "- Speeding up checks (some checks took 50 sec to run like TrainTestOverlap).\n",
    "\n",
    "- fixing the PR (logic, class inheritance, docstring).\n",
    "\n",
    "- First version of the quickstart available to use, which is simple and straightforward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create train and validation split.\n",
    "def split_train_validation(interaction_df : pd.DataFrame,\n",
    "                           session_col : str,\n",
    "                           item_col : str,\n",
    "                           timestamp_col : str, \n",
    "                           test_percentage=0.2,\n",
    "                           random_seed=None):\n",
    "    \n",
    "    assert set([session_col, item_col, timestamp_col]).issubset(interaction_df.columns)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    interaction_df = interaction_df.sort_values(timestamp_col,ascending=True)\n",
    "    split_index = int(len(interaction_df) * (1 - test_percentage))\n",
    "\n",
    "    train = interaction_df.iloc[:split_index]\n",
    "    test = interaction_df.iloc[split_index:]\n",
    "\n",
    "    # Let's discard overlapping sessions to make train and valid sets disjoints and independent.\n",
    "    overlapping_sessions = set(train[session_col]).intersection(set(test[session_col]))\n",
    "\n",
    "    test = test[~test[session_col].isin(overlapping_sessions)]\n",
    "\n",
    "    data_to_calculate_validation_score = []\n",
    "    new_test = []\n",
    "    for grp in test.groupby(session_col):\n",
    "        cutoff = np.random.randint(1, grp[1].shape[0]) # we want at least a single item in our validation data for each userId\n",
    "        new_test.append(grp[1].iloc[:cutoff])\n",
    "        data_to_calculate_validation_score.append(grp[1].iloc[cutoff:])\n",
    "\n",
    "    test = pd.concat(new_test).reset_index(drop=True)\n",
    "    \n",
    "    test_labels = pd.concat(data_to_calculate_validation_score).reset_index(drop=True)\n",
    "    assert test[timestamp_col].max() < test_labels[timestamp_col].max()\n",
    "    \n",
    "    test_labels = test_labels.groupby(session_col)[item_col].apply(list)\n",
    "    assert (test[session_col].unique() == test_labels.index.values).sum()\n",
    "    \n",
    "    return train,test,test_labels\n",
    "\n",
    "# Create user features.\n",
    "def user_features(X_interaction : pd.DataFrame):\n",
    "    user_df = pd.DataFrame()\n",
    "    user_df['userId'] = X_interaction['userId'].unique()\n",
    "    user_df['mean_rating'] = X_interaction.groupby(\"userId\")['rating'].mean().values\n",
    "    user_df['median_rating'] = X_interaction.groupby(\"userId\")['rating'].median().values\n",
    "    user_df['std_rating'] = X_interaction.groupby(\"userId\")['rating'].std().values\n",
    "    user_df['session_length'] = X_interaction.groupby(\"userId\")['rating'].count().values\n",
    "\n",
    "    user_df['min_rating'] = X_interaction.groupby(\"userId\")['rating'].min().values\n",
    "    user_df['max_rating'] = X_interaction.groupby(\"userId\")['rating'].max().values\n",
    "    user_df['last_timestamp'] = X_interaction.groupby(\"userId\")['timestamp'].tail(1).values\n",
    "    user_df['last_timestamp'] = user_df['last_timestamp'].apply(lambda x: pd.to_datetime(x).timestamp()).astype(int)\n",
    "\n",
    "    user_df['sum_rating'] = X_interaction.groupby(\"userId\")['rating'].sum().values\n",
    "\n",
    "\n",
    "    user_df['noise'] = np.random.normal(0,1,size=(user_df['session_length'].shape))\n",
    "\n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this guide, we\\'ll use a small subset of the [movieLens](https://grouplens.org/datasets/movielens/) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!unzip -n ml-latest-small.zip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "# Load interaction data.\n",
    "df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "\n",
    "# Load item data.\n",
    "movie_df =  pd.read_csv(\"ml-latest-small/movies.csv\")\n",
    "\n",
    "# Split interaction data into train and validation data.\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x : datetime.fromtimestamp(x))\n",
    "X_train_interactions, X_test_interactions, y_test = split_train_validation(interaction_df=df,\n",
    "                                           session_col='userId',\n",
    "                                           item_col ='movieId',\n",
    "                                           timestamp_col='timestamp',\n",
    "                                           test_percentage=0.6)\n",
    "\n",
    "# Create User Dataframe\n",
    "train_users_df = user_features(X_train_interactions)\n",
    "valid_users_df = user_features(X_test_interactions)\n",
    "\n",
    "# Add targets to the valid users\n",
    "valid_users_df = pd.merge(valid_users_df,y_test.rename('target'),how=\"left\",on=\"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recommender Datasets\n",
    "================================\n",
    "\n",
    "We can now create a Dataset object for the train and test dataframes. This object is\n",
    "used to pass your data to the deepchecks checks.\n",
    "\n",
    "To create a Recommender Dataset, the only required argument is the data\n",
    "itself, but passing only the data will prevent multiple checks from\n",
    "running. In this example we\\'ll define the task type\n",
    "and finally define the\n",
    "metadata columns (the other columns in the dataframe) which we\\'ll use later on in the\n",
    "guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender import InteractionDataset, UserDataset,ItemDataset\n",
    "\n",
    "# Interaction Datasets\n",
    "#################################################################\n",
    "train_interaction_ds = InteractionDataset(df=X_train_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "valid_interaction_ds = InteractionDataset(df=X_test_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "# User Datasets\n",
    "#################################################################\n",
    "train_user_ds = UserDataset(df = train_users_df,\n",
    "                label = None,                    \n",
    "                features=['mean_rating', 'session_length','median_rating','std_rating','noise','min_rating','max_rating','last_timestamp','sum_rating'],\n",
    "                cat_features=None)\n",
    "\n",
    "valid_user_ds = UserDataset(df = valid_users_df,\n",
    "                label = \"target\",                    \n",
    "                features=['mean_rating', 'session_length','median_rating','std_rating','noise','min_rating','max_rating','last_timestamp','sum_rating'],\n",
    "                cat_features=None)\n",
    "            \n",
    "# Item Dataset\n",
    "#################################################################\n",
    "item_ds = ItemDataset(df=movie_df,\n",
    "                      item_column_name='title',\n",
    "                      features=['title','genres'],\n",
    "                      cat_features=['title','genres'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recommender Model Class\n",
    "================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially used a covisitation recommender algorithm to generate a diverse set of candidate items for recommendation. This approach leverages the co-occurrence patterns of items in user sessions to suggest a pool of potential items that might be of interest to users.\n",
    "\n",
    "\n",
    "The co-occurrence recommender leverages the patterns of item co-occurrence in user sessions to generate relevant recommendations. It's based on the idea that if two items frequently appear together in user sessions, they might be related or have some inherent similarity that can be exploited for recommendations.\n",
    "\n",
    "\n",
    "1. **Fitting**: Using training data, we track associations between previous and current items based on their co-occurrence in user sessions.\n",
    "\n",
    "2. **Predicting**: For each user in validation data, we accumulate associated item counts from their session items.\n",
    "\n",
    "3. **Recommendations**: We suggest the most common associated items, excluding those already in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOccurrenceRecommender:\n",
    "    def __init__(self, user_col, item_col, num_predictions=20):\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.num_predictions = num_predictions\n",
    "        self.co_occurences = defaultdict(Counter)\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        # Make a copy of the training data\n",
    "        filtered_interactions = X_train.copy()\n",
    "        \n",
    "        # Create a new column with the previous item in each user session\n",
    "        prev_item_col = f'prev_{self.item_col}'\n",
    "        filtered_interactions[prev_item_col] = filtered_interactions.groupby(self.user_col)[self.item_col].shift(1).astype(\"Int64\").dropna()\n",
    "        \n",
    "        # Create a DataFrame with columns 'previous item' and 'item'\n",
    "        products_association_df = filtered_interactions[[prev_item_col, self.item_col]].copy().dropna()\n",
    "        \n",
    "        # Generate associations between 'previous item' and 'item'\n",
    "        for row in products_association_df.itertuples(index=False):\n",
    "            self.co_occurences[row[0]][row[1]] += 1\n",
    "\n",
    "    def predict(self, X_valid):\n",
    "        user_recommendations = {}\n",
    "\n",
    "        X_test_session_items = X_valid.groupby(self.user_col)[self.item_col].apply(list)\n",
    "\n",
    "        for user, items in X_test_session_items.items():\n",
    "            items = list(dict.fromkeys(items[::-1]))\n",
    "\n",
    "            counter = Counter()\n",
    "\n",
    "            for item in items:\n",
    "                subsequent_item_counter = self.co_occurences.get(item)\n",
    "                if subsequent_item_counter:\n",
    "                    counter += subsequent_item_counter\n",
    "            \n",
    "            # Get the top N recommended items based on the associations\n",
    "            recommendations = [item for item, cnt in counter.most_common(self.num_predictions) if item not in items]\n",
    "            user_recommendations[user] = recommendations\n",
    "        \n",
    "        recommendations_series = pd.Series(user_recommendations)\n",
    "        return recommendations_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, this method uses item co-occurrence patterns to provide recommendations by suggesting items that often appear together in user sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "recommender = CoOccurrenceRecommender(user_col='userId',\n",
    "                                      item_col='movieId',\n",
    "                                      num_predictions=50)\n",
    "\n",
    "# Fit the model on the training data\n",
    "recommender.fit(X_train_interactions)\n",
    "\n",
    "# Generate predictions for the validation data\n",
    "predictions = recommender.predict(X_test_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "from deepchecks.recommender.checks import SamplePerformance\n",
    "\n",
    "perf_check = SamplePerformance(scorers=[\n",
    "                                   'mean_average_recall_at_k',\n",
    "                                   'mean_average_precision_at_k',\n",
    "                                   'mean_average_f1_at_k',\n",
    "                                   'mean_average_ndcg_k',\n",
    "                                   'mean_reciprocal_rank'\n",
    "                                   ]\n",
    "                                )\n",
    "\n",
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=predictions.values.tolist())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DateTrainTestLeakageOverlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import DateTrainTestLeakageOverlap\n",
    "\n",
    "check = DateTrainTestLeakageOverlap(validation_per_user=False)\n",
    "\n",
    "result = check.run(train_dataset=train_interaction_ds,\n",
    "                   test_dataset=valid_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold Start Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks  import ColdStartDetection\n",
    "\n",
    "all_interaction_ds = train_interaction_ds + valid_interaction_ds\n",
    "check = ColdStartDetection()\n",
    "result = check.run(all_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product Association\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import ProductAssociation\n",
    "\n",
    "\n",
    "check = ProductAssociation(max_timestamp_delta=3600)\n",
    "result = check.run(all_interaction_ds,\n",
    "                   item_dataset=item_ds\n",
    "                   )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Session Length Drift\n",
    "===================================\n",
    "\n",
    "Also in the \\\"Didn\\'t Pass\\\" tab we can see the two segment performance\n",
    "checks - Property Segment Performance and Metadata Segment Performance.\n",
    "These use the metadata columns  of user related information OR our\n",
    "calculated properties to try and **automatically** detect significant data\n",
    "segments on which our model performs badly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import UserSessionDrift\n",
    "\n",
    "check = UserSessionDrift()\n",
    "\n",
    "result = check.run(train_dataset = train_interaction_ds,\n",
    "                   test_dataset = valid_interaction_ds)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Prediction Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import PredictionPopularityDrift\n",
    "\n",
    "check = PredictionPopularityDrift()\n",
    "result = check.run(valid_user_ds,\n",
    "                   y_pred=predictions.values.tolist(),\n",
    "                   interaction_dataset=train_interaction_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Label Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import LabelPopularityDrift\n",
    "\n",
    "check = LabelPopularityDrift()\n",
    "result = check.run(valid_user_ds,\n",
    "                   interaction_dataset=train_interaction_ds+valid_interaction_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import SegmentPerformance\n",
    "import traceback\n",
    "try:\n",
    "    result = SegmentPerformance(feature_1='session_length',\n",
    "                       feature_2='mean_rating',\n",
    "                       alternative_scorer={'recall':'mean_average_precision_at_k'},\n",
    "                       max_segments=3\n",
    "                       ).run(valid_user_ds, y_pred=predictions.values.tolist())\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeakSegmentPerformance\n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import WeakSegmentsPerformance\n",
    "\n",
    "check = WeakSegmentsPerformance(columns=['mean_rating',\n",
    "                                        'session_length',\n",
    "                                        'median_rating',\n",
    "                                        'std_rating',\n",
    "                                        'noise',\n",
    "                                        'min_rating',\n",
    "                                        'max_rating',\n",
    "                                        'last_timestamp',\n",
    "                                        'sum_rating'],\n",
    "                                alternative_scorer={'mean_average_recall_at_k': 'mean_average_recall_at_k'},\n",
    "                                segment_minimum_size_ratio=0.1,\n",
    "                                categorical_aggregation_threshold=0.5)\n",
    "result = check.run(valid_user_ds, y_pred=predictions.values.tolist())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate more candidates : a Word2Vec Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration of the co-occurrence recommender was just one facet of our strategy. In tandem, let's incorporate the Word2Vec model, which delves deeper into the semantic relationships between items. By mapping items into a multi-dimensional vector space, the Word2Vec model identifies underlying similarities between items, even when co-occurrence patterns are not explicit.\n",
    "\n",
    "\n",
    "\n",
    "To fit the Word2Vec model:\n",
    "\n",
    "1. **Data Preparation**: We combine training and validation data (without labels), grouping user interactions by the user column.\n",
    "\n",
    "2. **Creating Sentences**: User interactions become \"sentences\" for the Word2Vec model.\n",
    "\n",
    "3. **Training Word2Vec**: The model is trained using collected sentences.\n",
    "\n",
    "4. **Index Mapping**: We map item IDs to their Word2Vec indices.\n",
    "\n",
    "5. **Building K-NN Graph**: here we use ``annoy`` to build the knn-graph, which is suitable for large dataset, avoiding memory issues.\n",
    "\n",
    "6. **Adding Items to Graph**: For each item, we add its index and vector to the k-nearest neighbor graph.\n",
    "\n",
    "7. **Building K-NN Structure**: The graph is built for k-nearest neighbor queries.\n",
    "\n",
    "8. **Prediction**:  the recommendations will be the k-nearest item of the last item each user interacted with, using the knn-graph based on items embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "class Word2VecRecommender:\n",
    "    def __init__(self, user_col, item_col, vector_size=8, num_recommendations=20):\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.vector_size = vector_size\n",
    "        self.num_recommendations = num_recommendations\n",
    "        self.w2vec = None\n",
    "        self.knn_graph = None\n",
    "        self.item2idx = None\n",
    "    \n",
    "    def fit(self, X_train, X_valid):\n",
    "        sentences_df = pd.concat([X_train, X_valid]).groupby(self.user_col)[self.item_col].apply(list).reset_index()\n",
    "        sentences_df.rename(columns={self.item_col: 'sentence'}, inplace=True)\n",
    "\n",
    "        sentences = sentences_df['sentence'].to_list()\n",
    "\n",
    "        self.w2vec = Word2Vec(sentences=sentences, vector_size=self.vector_size, min_count=1)\n",
    "\n",
    "        self.item2idx = {aid: i for i, aid in enumerate(self.w2vec.wv.index_to_key)}\n",
    "\n",
    "        self.knn_graph = AnnoyIndex(self.vector_size, 'angular')\n",
    "\n",
    "        for aid, idx in self.item2idx.items():\n",
    "            self.knn_graph.add_item(idx, self.w2vec.wv.vectors[idx])\n",
    "\n",
    "        self.knn_graph.build(30)\n",
    "    \n",
    "    def predict(self, X_valid):\n",
    "        user_recommendations = {}\n",
    "\n",
    "        X_test_session_items = X_valid.groupby(self.user_col)[self.item_col].apply(list)\n",
    "\n",
    "        for user, items in X_test_session_items.items():\n",
    "            items = list(dict.fromkeys(items[::-1]))\n",
    "\n",
    "            most_recent_aid = items[0]\n",
    "\n",
    "            nns = [self.w2vec.wv.index_to_key[i] for i in \n",
    "                   self.knn_graph.get_nns_by_item(self.item2idx[most_recent_aid], self.num_recommendations + 1)[1:]]\n",
    "\n",
    "            recommendations = [item for item in nns if item not in items]\n",
    "            user_recommendations[user] = recommendations\n",
    "\n",
    "        word2vec_recommendations = pd.Series(user_recommendations)\n",
    "        return word2vec_recommendations\n",
    "\n",
    "word2vec_recommender = Word2VecRecommender(user_col='userId',\n",
    "                                           item_col='movieId',\n",
    "                                           vector_size=8,\n",
    "                                           num_recommendations=50)\n",
    "word2vec_recommender.fit(X_train_interactions, X_test_interactions)\n",
    "word2vec_predictions = word2vec_recommender.predict(X_test_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Performance\n",
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=word2vec_predictions.values.tolist())\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine predictions/candidates\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = pd.concat([predictions.rename(\"co-occurence_pred\"),word2vec_predictions.rename(\"word2vec_pred\")],axis=1)\n",
    "all_predictions['all_preds'] = all_predictions['co-occurence_pred'] + all_predictions['word2vec_pred']\n",
    "all_predictions['all_preds'] = all_predictions['all_preds'].apply(lambda x : list(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking : a LightGBMRanker Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating a large set of candidate items, let's use the LGBMRanker algorithm for reranking. \n",
    "\n",
    "LGBMRanker is a gradient boosting algorithm designed for ranking tasks, and it's well-suited for reranking a list of items based on their predicted relevance to users.\n",
    "\n",
    "- The reranking process involves considering multiple features or signals associated with items and users to estimate the relevance of items for individual users. The algorithm takes into account various factors such as item popularity, user behavior, and more.\n",
    "\n",
    "- By reranking the candidates using LGBMRanker, we will reorder the candidate items for each user in a way that aims to improve the overall relevance of the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess item features\n",
    "item_popularity = pd.concat([X_train_interactions,X_test_interactions],axis=0)['movieId'].value_counts().to_dict()\n",
    "\n",
    "def preprocess_item_features(movie_df, item_popularity):\n",
    "    movie_df[['genre_1', 'genre_2', 'genre_3']] = movie_df['genres'].str.split('|', expand=True).iloc[:, :3]\n",
    "    popularity_df = pd.DataFrame(list(item_popularity.items()), columns=['movieId', 'popularity'])\n",
    "    item_features = movie_df.merge(popularity_df, on='movieId').drop(['title', 'genres'], axis=1)\n",
    "    return item_features\n",
    "\n",
    "item_features = preprocess_item_features(movie_df, item_popularity)\n",
    "\n",
    "# Preprocess user features\n",
    "def preprocess_user_features(valid_users_df):\n",
    "    user_features = valid_users_df.copy().drop('target', axis=1)\n",
    "    return user_features\n",
    "\n",
    "user_features = preprocess_user_features(valid_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode candidates and true labels\n",
    "def explode_candidates_true_labels(predictions, y_test, item_features, user_features):\n",
    "    candidates = predictions.reset_index()\n",
    "    candidates.columns = ['userId', 'item']\n",
    "    candidates = candidates.explode('item')\n",
    "\n",
    "    df = candidates.merge(item_features, left_on='item', right_on='movieId', right_index=True, how='left').fillna(-1)\n",
    "    df = df.merge(user_features, on='userId', how='left').fillna(-1)\n",
    "\n",
    "    true_labels = y_test.reset_index()\n",
    "    true_labels.columns = ['userId', 'item']\n",
    "    true_labels = true_labels.explode('item')\n",
    "    true_labels['gt'] = 1\n",
    "\n",
    "    df_ = pd.merge(df, true_labels, on=['userId', 'item'], how='left').fillna(0)\n",
    "    df_['gt'] = df_['gt'].astype(int)\n",
    "    \n",
    "    object_columns = df_.select_dtypes(include=['object']).columns\n",
    "    df_[object_columns] = df_[object_columns].astype('category')\n",
    "\n",
    "    return df_\n",
    "\n",
    "df_ = explode_candidates_true_labels(all_predictions['all_preds'], y_test, item_features, user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "# Splitting the data into train and validation\n",
    "def split_data(df_):\n",
    "    unique_users = df_['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "    train_size = int(len(unique_users) * 0.7)\n",
    "    train_users = unique_users[:train_size]\n",
    "    valid_users = unique_users[train_size:]\n",
    "    train_df = df_[df_['userId'].isin(train_users)]\n",
    "    valid_df = df_[df_['userId'].isin(valid_users)]\n",
    "\n",
    "    d_train = train_df.groupby(\"userId\").size().values.tolist()\n",
    "    d_valid = valid_df.groupby(\"userId\").size().values.tolist()\n",
    "\n",
    "    return train_df, valid_df, d_train, d_valid\n",
    "\n",
    "train_df, valid_df, d_train, d_valid = split_data(df_)\n",
    "\n",
    "\n",
    "# Training and evaluation\n",
    "def train_evaluate_lgbm_ranker(train_df, valid_df, features_col, target_col, d_train, d_valid):\n",
    "    ranker = LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "    )\n",
    "    n_eval = int(df_.groupby(\"userId\").size().mean())\n",
    "    ranker.fit(\n",
    "        train_df[features_col],\n",
    "        train_df[target_col],\n",
    "        group=d_train,\n",
    "        eval_set=[(valid_df[features_col], valid_df[target_col])],\n",
    "        eval_group=[d_valid],\n",
    "        eval_metric=\"ndcg\",\n",
    "        eval_at=[int(n_eval/3), int(n_eval/2),n_eval]\n",
    "    )\n",
    "\n",
    "    return ranker, ranker.best_score_['valid_0']\n",
    "\n",
    "features_col = ['genre_1', 'genre_2', 'genre_3',\n",
    "       'popularity', 'mean_rating', 'median_rating', 'std_rating',\n",
    "       'session_length', 'min_rating', 'max_rating', 'last_timestamp',\n",
    "       'sum_rating', 'noise']\n",
    "target_col = ['gt']\n",
    "\n",
    "ranker, best_ndcg_score = train_evaluate_lgbm_ranker(train_df, valid_df, features_col, target_col, d_train, d_valid)\n",
    "print(f\"Best NDCG Score: {best_ndcg_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions\n",
    "def generate_predictions(ranker, candidates, features_col):\n",
    "    scores = ranker.predict(candidates[features_col])\n",
    "\n",
    "    candidates['score'] = scores\n",
    "    predictions_lgbm = (\n",
    "        candidates.sort_values(by=['userId', 'score'], ascending=[True, False])\n",
    "        .groupby('userId')\n",
    "        .apply(lambda group: group['item'].head(20).tolist())\n",
    "    )\n",
    "    return predictions_lgbm\n",
    "\n",
    "predictions_lgbm = generate_predictions(ranker, df_, features_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=predictions_lgbm.values.tolist())\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the results, using a reranker algorithm like LGBMRanker enhances the performance of our recommender system.\n",
    "\n",
    "\n",
    "In summary, we've combined the strengths of both the covisitation recommender, which captures item co-occurrence patterns, and the Word2Vec model, which captures item semantics, to generate an extensive list of candidate items. Then, by using the LGBMRanker algorithm to rerank these candidates, we've achieved better recommendations by considering multiple factors related to item-user interactions and relevance. This approach reflects a well-rounded and effective strategy for improving the recommendation quality of our system."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
