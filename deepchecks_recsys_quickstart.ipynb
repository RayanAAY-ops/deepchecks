{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Deepchecks for Recommender System\n",
    "===============================================================================\n",
    "\n",
    "Deepchecks Recommender  is your go-to tool for developing and evaluating recommender system models, ensuring their robustness before deployment. Our comprehensive testing package not only detects potential failures but also saves you valuable development time. In this quickstart guide, you'll learn how to utilize Deepchecks Recommender to analyze and evaluate various aspects of your recommender system, including data quality, leakage, product associations, cold start detection, and drift. Let's get started.\n",
    "\n",
    "**Step 1: Data Preparation and Auto Analysis**\n",
    "---------------------------------------------\n",
    "\n",
    "To run Deepchecks Recommender, make sure you have the following data for both your training and testing sets:\n",
    "\n",
    "1. User-Item Interaction Data: A structured dataset containing information about user-item interactions. Each record represents a user's interaction with an item, such as viewing, purchasing, or rating.\n",
    "\n",
    "2. Product Information: Additional information about the items in your catalog, like product categories, descriptions, or features.\n",
    "\n",
    "3. User Information (Optional): If available, user-specific data such as demographics, preferences, or historical behavior can enhance the evaluation.\n",
    "\n",
    "4. Your labels : These are not needed for checks that don't require labels (such as the Cold Start Detection check or most data integrity checks), but are needed for many other checks.\n",
    "\n",
    "5. Your model's predictions: These are needed only for the model related checks, shown in the Model Evaluation section of this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of Libraries\n",
    "=========================\n",
    "we will install two essential Python libraries, gensim and annoy, to empower our recommender system.\n",
    "\n",
    "`gensim` is a popular library for topic modeling and document similarity analysis. It provides tools for training and using word embeddings, topic models, and other natural language processing techniques.\n",
    "\n",
    "`annoy` is a library for approximate nearest neighbor search. It is often used in information retrieval and recommendation systems to efficiently find similar items in large datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install gensim==4.3.1 annoy==1.17.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create train and validation split.\n",
    "def split_train_validation(interaction_df : pd.DataFrame,\n",
    "                           session_col : str,\n",
    "                           item_col : str,\n",
    "                           timestamp_col : str, \n",
    "                           test_percentage=0.2,\n",
    "                           random_seed=None):\n",
    "    \n",
    "    assert set([session_col, item_col, timestamp_col]).issubset(interaction_df.columns)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    interaction_df = interaction_df.sort_values(timestamp_col,ascending=True)\n",
    "    split_index = int(len(interaction_df) * (1 - test_percentage))\n",
    "\n",
    "    train = interaction_df.iloc[:split_index]\n",
    "    test = interaction_df.iloc[split_index:]\n",
    "\n",
    "    # Let's discard overlapping sessions to make train and valid sets disjoints and independent.\n",
    "    overlapping_sessions = set(train[session_col]).intersection(set(test[session_col]))\n",
    "\n",
    "    test = test[~test[session_col].isin(overlapping_sessions)]\n",
    "\n",
    "    data_to_calculate_validation_score = []\n",
    "    new_test = []\n",
    "    for grp in test.groupby(session_col):\n",
    "        cutoff = np.random.randint(1, grp[1].shape[0]) # we want at least a single item in our validation data for each userId\n",
    "        new_test.append(grp[1].iloc[:cutoff])\n",
    "        data_to_calculate_validation_score.append(grp[1].iloc[cutoff:])\n",
    "\n",
    "    test = pd.concat(new_test).reset_index(drop=True)\n",
    "    \n",
    "    test_labels = pd.concat(data_to_calculate_validation_score).reset_index(drop=True)\n",
    "    assert test[timestamp_col].max() < test_labels[timestamp_col].max()\n",
    "    \n",
    "    test_labels = test_labels.groupby(session_col)[item_col].apply(list)\n",
    "    assert (test[session_col].unique() == test_labels.index.values).sum()\n",
    "    \n",
    "    return train,test,test_labels\n",
    "\n",
    "# Create user features.\n",
    "def user_features(X_interaction : pd.DataFrame):\n",
    "    user_df = pd.DataFrame()\n",
    "    user_df['userId'] = X_interaction['userId'].unique()\n",
    "    user_df['mean_rating'] = X_interaction.groupby(\"userId\")['rating'].mean().values\n",
    "    user_df['median_rating'] = X_interaction.groupby(\"userId\")['rating'].median().values\n",
    "    user_df['std_rating'] = X_interaction.groupby(\"userId\")['rating'].std().values\n",
    "    user_df['session_length'] = X_interaction.groupby(\"userId\")['rating'].count().values\n",
    "\n",
    "    user_df['min_rating'] = X_interaction.groupby(\"userId\")['rating'].min().values\n",
    "    user_df['max_rating'] = X_interaction.groupby(\"userId\")['rating'].max().values\n",
    "    user_df['last_timestamp'] = X_interaction.groupby(\"userId\")['timestamp'].tail(1).values\n",
    "    user_df['last_timestamp'] = user_df['last_timestamp'].apply(lambda x: pd.to_datetime(x).timestamp()).astype(int)\n",
    "\n",
    "    user_df['sum_rating'] = X_interaction.groupby(\"userId\")['rating'].sum().values\n",
    "\n",
    "\n",
    "    user_df['noise'] = np.random.normal(0,1,size=(user_df['session_length'].shape))\n",
    "\n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this guide, we\\'ll use a small subset of the [movieLens](https://grouplens.org/datasets/movielens/) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\n",
    "!unzip -n ml-latest-small.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "# Load interaction data.\n",
    "df = pd.read_csv(\"ml-latest-small/ratings.csv\")\n",
    "\n",
    "# Load item data.\n",
    "movie_df =  pd.read_csv(\"ml-latest-small/movies.csv\")\n",
    "\n",
    "# Split interaction data into train and validation data.\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x : datetime.fromtimestamp(x))\n",
    "X_train_interactions, X_test_interactions, y_test = split_train_validation(interaction_df=df,\n",
    "                                           session_col='userId',\n",
    "                                           item_col ='movieId',\n",
    "                                           timestamp_col='timestamp',\n",
    "                                           test_percentage=0.6)\n",
    "\n",
    "# Create User Dataframe\n",
    "train_users_df = user_features(X_train_interactions)\n",
    "valid_users_df = user_features(X_test_interactions)\n",
    "\n",
    "# Add targets to the valid users\n",
    "valid_users_df = pd.merge(valid_users_df,y_test.rename('target'),how=\"left\",on=\"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Create a Recommender Datasets\n",
    "================================\n",
    "\n",
    "We can now create a Dataset object for the train and test dataframes. This object is\n",
    "used to pass your data to the deepchecks checks.\n",
    "\n",
    "To create a Recommender Dataset, the only required argument is the data\n",
    "itself, but passing only the data will prevent multiple checks from\n",
    "running. In this example we\\'ll define the task type\n",
    "and finally define the\n",
    "metadata columns (the other columns in the dataframe) which we\\'ll use later on in the\n",
    "guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender import InteractionDataset, UserDataset,ItemDataset\n",
    "\n",
    "# Interaction Datasets\n",
    "#################################################################\n",
    "train_interaction_ds = InteractionDataset(df=X_train_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "valid_interaction_ds = InteractionDataset(df=X_test_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "# User Datasets\n",
    "#################################################################\n",
    "train_user_ds = UserDataset(df = train_users_df,\n",
    "                label = None,                    \n",
    "                features=['mean_rating', 'session_length','median_rating','std_rating','noise','min_rating','max_rating','last_timestamp','sum_rating'],\n",
    "                cat_features=None)\n",
    "\n",
    "valid_user_ds = UserDataset(df = valid_users_df,\n",
    "                label = \"target\",                    \n",
    "                features=['mean_rating', 'session_length','median_rating','std_rating','noise','min_rating','max_rating','last_timestamp','sum_rating'],\n",
    "                cat_features=None)\n",
    "            \n",
    "# Item Dataset\n",
    "#################################################################\n",
    "item_ds = ItemDataset(df=movie_df,\n",
    "                      item_column_name='title',\n",
    "                      features=['title','genres'],\n",
    "                      cat_features=['title','genres'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recommender Model Class\n",
    "================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initially used a covisitation recommender algorithm to generate a diverse set of candidate items for recommendation. This approach leverages the co-occurrence patterns of items in user sessions to suggest a pool of potential items that might be of interest to users.\n",
    "\n",
    "\n",
    "The co-occurrence recommender leverages the patterns of item co-occurrence in user sessions to generate relevant recommendations. It's based on the idea that if two items frequently appear together in user sessions, they might be related or have some inherent similarity that can be exploited for recommendations.\n",
    "\n",
    "\n",
    "1. **Fitting**: Using training data, we track associations between previous and current items based on their co-occurrence in user sessions.\n",
    "\n",
    "2. **Predicting**: For each user in validation data, we accumulate associated item counts from their session items.\n",
    "\n",
    "3. **Recommendations**: We suggest the most common associated items, excluding those already in the session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOccurrenceRecommender:\n",
    "    def __init__(self, user_col, item_col, num_predictions=20):\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.num_predictions = num_predictions\n",
    "        self.co_occurences = defaultdict(Counter)\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        # Make a copy of the training data\n",
    "        filtered_interactions = X_train.copy()\n",
    "        \n",
    "        # Create a new column with the previous item in each user session\n",
    "        prev_item_col = f'prev_{self.item_col}'\n",
    "        filtered_interactions[prev_item_col] = filtered_interactions.groupby(self.user_col)[self.item_col].shift(1).astype(\"Int64\").dropna()\n",
    "        \n",
    "        # Create a DataFrame with columns 'previous item' and 'item'\n",
    "        products_association_df = filtered_interactions[[prev_item_col, self.item_col]].copy().dropna()\n",
    "        \n",
    "        # Generate associations between 'previous item' and 'item'\n",
    "        for row in products_association_df.itertuples(index=False):\n",
    "            self.co_occurences[row[0]][row[1]] += 1\n",
    "\n",
    "    def predict(self, X_valid):\n",
    "        user_recommendations = {}\n",
    "\n",
    "        X_test_session_items = X_valid.groupby(self.user_col)[self.item_col].apply(list)\n",
    "\n",
    "        for user, items in X_test_session_items.items():\n",
    "            items = list(dict.fromkeys(items[::-1]))\n",
    "\n",
    "            counter = Counter()\n",
    "\n",
    "            for item in items:\n",
    "                subsequent_item_counter = self.co_occurences.get(item)\n",
    "                if subsequent_item_counter:\n",
    "                    counter += subsequent_item_counter\n",
    "            \n",
    "            # Get the top N recommended items based on the associations\n",
    "            recommendations = [item for item, cnt in counter.most_common(self.num_predictions) if item not in items]\n",
    "            user_recommendations[user] = recommendations\n",
    "        \n",
    "        recommendations_series = pd.Series(user_recommendations)\n",
    "        return recommendations_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary, this method uses item co-occurrence patterns to provide recommendations by suggesting items that often appear together in user sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "recommender = CoOccurrenceRecommender(user_col='userId',\n",
    "                                      item_col='movieId',\n",
    "                                      num_predictions=50)\n",
    "\n",
    "# Fit the model on the training data\n",
    "recommender.fit(X_train_interactions)\n",
    "\n",
    "# Generate predictions for the validation data\n",
    "predictions = recommender.predict(X_test_interactions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SamplePerformance\n",
    "===================\n",
    "This evaluation check is tailored for assessing the performance of a recommender system on a labeled dataset using various metrics.\n",
    "\n",
    "In the context of recommender systems, we employ specific metrics to evaluate the model's effectiveness. These metrics gauge the quality of recommendations provided by the system. Just as scorers are a convention in sklearn for model evaluation, these metrics are standard in the realm of recommender systems.\n",
    "\n",
    "The default metrics we employ for recommender systems encompass Mean Average Recall (MAR), Precision (MAP), F1-Score (MA F1), Normalized Discounted Cumulative Gain (NDCG), mean reciprocal rank (MRR). Each of these metrics serves to quantify different aspects of recommendation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import SamplePerformance\n",
    "\n",
    "perf_check = SamplePerformance(scorers=[\n",
    "                                   'mean_average_recall_at_k',\n",
    "                                   'mean_average_precision_at_k',\n",
    "                                   'mean_average_f1_at_k',\n",
    "                                   'mean_average_ndcg_k',\n",
    "                                   'mean_reciprocal_rank'\n",
    "                                   ],\n",
    "                                   k = 20\n",
    "                                )\n",
    "perf_check.add_condition_greater_than(threshold=0.1, class_mode='all')\n",
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=predictions.values.tolist())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DateTrainTestLeakageOverlap\n",
    "============================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage and user independence between the train and test datasets is crucial in ensuring **the reliability of recommender systems**. Here's a concise version that retains the key points:\n",
    "\n",
    "Data leakage and user independence is critical in recommender systems. These safeguards prevent inadvertent information transfer between training and testing data and verify that users in the training set are distinct from those in the test set. By doing so, the system maintains its:\n",
    "- integrity.\n",
    "- accuracy.\n",
    "- unbiased performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import DateTrainTestLeakageOverlap\n",
    "\n",
    "check = DateTrainTestLeakageOverlap(validation_per_user=False)\n",
    "\n",
    "result = check.run(train_dataset=train_interaction_ds,\n",
    "                   test_dataset=valid_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cold Start Detection\n",
    "====================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting cold start users is vital in recommender systems due to their scarce historical data. This hampers personalization, user engagement, and system performance, leading to sparse data challenges. By addressing cold start users, systems can enhance personalization, user onboarding, overall performance, and business outcomes, ensuring effective recommendations and user satisfaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks  import ColdStartDetection\n",
    "\n",
    "all_interaction_ds = train_interaction_ds + valid_interaction_ds\n",
    "check = ColdStartDetection()\n",
    "check.add_condition_greater_than(min_cold_start_entity=0.1)\n",
    "\n",
    "result = check.run(all_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product Association\n",
    "====================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product association is a fundamental concept within recommender systems, focusing on understanding the connections and patterns of products that tend to be chosen together by consumers. This notion holds significant importance in optimizing the performance of machine learning models.\n",
    "\n",
    "Product association dynamics come into play when shifts in consumer behavior lead to changes in the correlations between items frequently purchased or recommended in tandem. Recognizing and analyzing product associations is particularly advantageous when there is a lack of available labels for the test dataset\n",
    "\n",
    "*Let say the probability of buying product X (e.g., ketchup) in a supermarket is 10%,\n",
    "and product Y (e.g., ground beef) is 20%, then if we assume independence, the probability\n",
    "of both happening together would be P(X) * P(Y) = 2%.\n",
    "However, if the probability of both products occurring together is 8%,resulting in a lift of 4,\n",
    "it means that those products are four times more likely to be purchased together\n",
    "than if they had no relationship to each other.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import ProductAssociation\n",
    "\n",
    "check = ProductAssociation(max_timestamp_delta=3600)\n",
    "result = check.run(all_interaction_ds,\n",
    "                   item_dataset=item_ds\n",
    "                   )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drift Checks\n",
    "=============="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Popularity drift is a phenomenon within recommender systems where the relative popularity of items changes over time. This dynamic shift in item popularity significantly impacts the performance of machine learning models.\n",
    "\n",
    "Popularity drift occurs when the distribution of item preferences or choices evolves, leading to changes in the popularity ranking of items. Recognizing and quantifying popularity drift is particularly valuable when there is a lack of available labels for the test dataset. In such cases, observing a drift in the predicted popularities serves as the primary signal that shifts have transpired in the underlying data affecting the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Session Length Drift\n",
    "===================================\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import UserSessionDrift\n",
    "# Explain user session drift\n",
    "check = UserSessionDrift()\n",
    "check.add_condition_drift_score_less_than()\n",
    "result = check.run(train_dataset = train_interaction_ds,\n",
    "                   test_dataset = valid_interaction_ds)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Prediction Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import PredictedItemsPopularityDrift\n",
    "\n",
    "check = PredictedItemsPopularityDrift()\n",
    "check.add_condition_drift_score_less_than()\n",
    "result = check.run(valid_user_ds,\n",
    "                   y_pred=predictions.values.tolist(),\n",
    "                   interaction_dataset=train_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Label Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import LabelPopularityDrift\n",
    "\n",
    "check = LabelPopularityDrift()\n",
    "check.add_condition_drift_score_less_than()\n",
    "\n",
    "result = check.run(valid_user_ds,\n",
    "                   interaction_dataset=train_interaction_ds+valid_interaction_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segment Performance\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import SegmentPerformance\n",
    "import traceback\n",
    "try:\n",
    "    result = SegmentPerformance(feature_1='session_length',\n",
    "                       feature_2='mean_rating',\n",
    "                       alternative_scorer={'recall':'mean_average_precision_at_k'},\n",
    "                       max_segments=3\n",
    "                       ).run(valid_user_ds, y_pred=predictions.values.tolist())\n",
    "except:\n",
    "    traceback.print_exc()\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WeakSegmentPerformance\n",
    "----------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The check is designed to help you easily identify the model’s weakest segments in the data provided. In addition, it enables to provide a sublist of the Dataset’s features, thus limiting the check to search in interesting subspaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import WeakSegmentsPerformance\n",
    "\n",
    "check = WeakSegmentsPerformance(columns=['mean_rating',\n",
    "                                        'session_length',\n",
    "                                        'median_rating',\n",
    "                                        'std_rating',\n",
    "                                        'noise',\n",
    "                                        'min_rating',\n",
    "                                        'max_rating',\n",
    "                                        'last_timestamp',\n",
    "                                        'sum_rating'],\n",
    "                                alternative_scorer={'mean_average_recall_at_k': 'mean_average_recall_at_k'},\n",
    "                                segment_minimum_size_ratio=0.1,\n",
    "                                categorical_aggregation_threshold=0.5)\n",
    "check.add_condition_segments_relative_performance_greater_than(0.1)\n",
    "\n",
    "result = check.run(valid_user_ds, y_pred=predictions.values.tolist())\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate more candidates : a Word2Vec Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The integration of the co-occurrence recommender was just one facet of our strategy. In tandem, let's incorporate the Word2Vec model, which delves deeper into the semantic relationships between items. By mapping items into a multi-dimensional vector space, the Word2Vec model identifies underlying similarities between items, even when co-occurrence patterns are not explicit.\n",
    "\n",
    "\n",
    "\n",
    "To fit the Word2Vec model:\n",
    "\n",
    "1. **Data Preparation**: We combine training and validation data (without labels), grouping user interactions by the user column.\n",
    "\n",
    "2. **Creating Sentences**: User interactions become \"sentences\" for the Word2Vec model.\n",
    "\n",
    "3. **Training Word2Vec**: The model is trained using collected sentences.\n",
    "\n",
    "4. **Index Mapping**: We map item IDs to their Word2Vec indices.\n",
    "\n",
    "5. **Building K-NN Graph**: here we use ``annoy`` to build the knn-graph, which is suitable for large dataset, avoiding memory issues.\n",
    "\n",
    "6. **Adding Items to Graph**: For each item, we add its index and vector to the k-nearest neighbor graph.\n",
    "\n",
    "7. **Building K-NN Structure**: The graph is built for k-nearest neighbor queries.\n",
    "\n",
    "8. **Prediction**:  the recommendations will be the k-nearest item of the last item each user interacted with, using the knn-graph based on items embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from annoy import AnnoyIndex\n",
    "\n",
    "class Word2VecRecommender:\n",
    "    def __init__(self, user_col, item_col, vector_size=8, num_recommendations=20):\n",
    "        self.user_col = user_col\n",
    "        self.item_col = item_col\n",
    "        self.vector_size = vector_size\n",
    "        self.num_recommendations = num_recommendations\n",
    "        self.w2vec = None\n",
    "        self.knn_graph = None\n",
    "        self.item2idx = None\n",
    "    \n",
    "    def fit(self, X_train, X_valid):\n",
    "        sentences_df = pd.concat([X_train, X_valid]).groupby(self.user_col)[self.item_col].apply(list).reset_index()\n",
    "        sentences_df.rename(columns={self.item_col: 'sentence'}, inplace=True)\n",
    "\n",
    "        sentences = sentences_df['sentence'].to_list()\n",
    "\n",
    "        self.w2vec = Word2Vec(sentences=sentences, vector_size=self.vector_size, min_count=1)\n",
    "\n",
    "        self.item2idx = {aid: i for i, aid in enumerate(self.w2vec.wv.index_to_key)}\n",
    "\n",
    "        self.knn_graph = AnnoyIndex(self.vector_size, 'angular')\n",
    "\n",
    "        for aid, idx in self.item2idx.items():\n",
    "            self.knn_graph.add_item(idx, self.w2vec.wv.vectors[idx])\n",
    "\n",
    "        self.knn_graph.build(30)\n",
    "    \n",
    "    def predict(self, X_valid):\n",
    "        user_recommendations = {}\n",
    "\n",
    "        X_test_session_items = X_valid.groupby(self.user_col)[self.item_col].apply(list)\n",
    "\n",
    "        for user, items in X_test_session_items.items():\n",
    "            items = list(dict.fromkeys(items[::-1]))\n",
    "\n",
    "            most_recent_aid = items[0]\n",
    "\n",
    "            nns = [self.w2vec.wv.index_to_key[i] for i in \n",
    "                   self.knn_graph.get_nns_by_item(self.item2idx[most_recent_aid], self.num_recommendations + 1)[1:]]\n",
    "\n",
    "            recommendations = [item for item in nns if item not in items]\n",
    "            user_recommendations[user] = recommendations\n",
    "\n",
    "        word2vec_recommendations = pd.Series(user_recommendations)\n",
    "        return word2vec_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time \n",
    "\n",
    "word2vec_recommender = Word2VecRecommender(user_col='userId',\n",
    "                                           item_col='movieId',\n",
    "                                           vector_size=8,\n",
    "                                           num_recommendations=50)\n",
    "word2vec_recommender.fit(X_train_interactions, X_test_interactions)\n",
    "word2vec_predictions = word2vec_recommender.predict(X_test_interactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Performance\n",
    "result = perf_check.run(valid_user_ds,\n",
    "                   y_pred=word2vec_predictions.values.tolist())\n",
    "\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine predictions/candidates\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_predictions = pd.concat([predictions.rename(\"co-occurence_pred\"),word2vec_predictions.rename(\"word2vec_pred\")],axis=1)\n",
    "all_predictions['all_preds'] = all_predictions['co-occurence_pred'] + all_predictions['word2vec_pred']\n",
    "all_predictions['all_preds'] = all_predictions['all_preds'].apply(lambda x : list(set(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reranking : a LightGBMRanker Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After generating a large set of candidate items, let's use the LGBMRanker algorithm for reranking. \n",
    "\n",
    "LGBMRanker is a gradient boosting algorithm designed for ranking tasks, and it's well-suited for reranking a list of items based on their predicted relevance to users.\n",
    "\n",
    "- The reranking process involves considering multiple features or signals associated with items and users to estimate the relevance of items for individual users. The algorithm takes into account various factors such as item popularity, user behavior, and more.\n",
    "\n",
    "- By reranking the candidates using LGBMRanker, we will reorder the candidate items for each user in a way that aims to improve the overall relevance of the recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess item features\n",
    "item_popularity = pd.concat([X_train_interactions,X_test_interactions],axis=0)['movieId'].value_counts().to_dict()\n",
    "\n",
    "def preprocess_item_features(movie_df, item_popularity):\n",
    "    movie_df[['genre_1', 'genre_2', 'genre_3']] = movie_df['genres'].str.split('|', expand=True).iloc[:, :3]\n",
    "    popularity_df = pd.DataFrame(list(item_popularity.items()), columns=['movieId', 'popularity'])\n",
    "    item_features = movie_df.merge(popularity_df, on='movieId').drop(['title', 'genres'], axis=1)\n",
    "    return item_features\n",
    "\n",
    "item_features = preprocess_item_features(movie_df, item_popularity)\n",
    "\n",
    "# Preprocess user features\n",
    "def preprocess_user_features(valid_users_df):\n",
    "    user_features = valid_users_df.copy().drop('target', axis=1)\n",
    "    return user_features\n",
    "\n",
    "user_features = preprocess_user_features(valid_users_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explode candidates and true labels\n",
    "def explode_candidates_true_labels(predictions, y_test, item_features, user_features):\n",
    "    candidates = predictions.reset_index()\n",
    "    candidates.columns = ['userId', 'item']\n",
    "    candidates = candidates.explode('item')\n",
    "\n",
    "    df = candidates.merge(item_features, left_on='item', right_on='movieId', right_index=True, how='left').fillna(-1)\n",
    "    df = df.merge(user_features, on='userId', how='left').fillna(-1)\n",
    "\n",
    "    true_labels = y_test.reset_index()\n",
    "    true_labels.columns = ['userId', 'item']\n",
    "    true_labels = true_labels.explode('item')\n",
    "    true_labels['gt'] = 1\n",
    "\n",
    "    df_ = pd.merge(df, true_labels, on=['userId', 'item'], how='left').fillna(0)\n",
    "    df_['gt'] = df_['gt'].astype(int)\n",
    "    \n",
    "    object_columns = df_.select_dtypes(include=['object']).columns\n",
    "    df_[object_columns] = df_[object_columns].astype('category')\n",
    "\n",
    "    return df_\n",
    "\n",
    "df_ = explode_candidates_true_labels(all_predictions['all_preds'], y_test, item_features, user_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm.sklearn import LGBMRanker\n",
    "\n",
    "# Splitting the data into train and validation\n",
    "def split_data(df_):\n",
    "    unique_users = df_['userId'].unique()\n",
    "    np.random.shuffle(unique_users)\n",
    "    train_size = int(len(unique_users) * 0.7)\n",
    "    train_users = unique_users[:train_size]\n",
    "    valid_users = unique_users[train_size:]\n",
    "    train_df = df_[df_['userId'].isin(train_users)]\n",
    "    valid_df = df_[df_['userId'].isin(valid_users)]\n",
    "\n",
    "    d_train = train_df.groupby(\"userId\").size().values.tolist()\n",
    "    d_valid = valid_df.groupby(\"userId\").size().values.tolist()\n",
    "\n",
    "    return train_df, valid_df, d_train, d_valid\n",
    "\n",
    "train_df, valid_df, d_train, d_valid = split_data(df_)\n",
    "\n",
    "\n",
    "# Training and evaluation\n",
    "def train_evaluate_lgbm_ranker(train_df, valid_df, features_col, target_col, d_train, d_valid):\n",
    "    ranker = LGBMRanker(\n",
    "        objective=\"lambdarank\",\n",
    "        metric=\"ndcg\",\n",
    "        boosting_type=\"gbdt\",\n",
    "        n_estimators=100,\n",
    "        max_depth=20,\n",
    "    )\n",
    "    n_eval = int(df_.groupby(\"userId\").size().mean())\n",
    "    ranker.fit(\n",
    "        train_df[features_col],\n",
    "        train_df[target_col],\n",
    "        group=d_train,\n",
    "        eval_set=[(valid_df[features_col], valid_df[target_col])],\n",
    "        eval_group=[d_valid],\n",
    "        eval_metric=\"ndcg\",\n",
    "        eval_at=[int(n_eval/3), int(n_eval/2),n_eval]\n",
    "    )\n",
    "\n",
    "    return ranker, ranker.best_score_['valid_0']\n",
    "\n",
    "features_col = ['genre_1', 'genre_2', 'genre_3',\n",
    "       'popularity', 'mean_rating', 'median_rating', 'std_rating',\n",
    "       'session_length', 'min_rating', 'max_rating', 'last_timestamp',\n",
    "       'sum_rating', 'noise']\n",
    "target_col = ['gt']\n",
    "\n",
    "ranker, best_ndcg_score = train_evaluate_lgbm_ranker(train_df, valid_df, features_col, target_col, d_train, d_valid)\n",
    "print(f\"Best NDCG Score: {best_ndcg_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating predictions\n",
    "def generate_predictions(ranker, candidates, features_col):\n",
    "    scores = ranker.predict(candidates[features_col])\n",
    "\n",
    "    candidates['score'] = scores\n",
    "    predictions_lgbm = (\n",
    "        candidates.sort_values(by=['userId', 'score'], ascending=[True, False])\n",
    "        .groupby('userId')\n",
    "        .apply(lambda group: group['item'].head(20).tolist())\n",
    "    )\n",
    "    return predictions_lgbm\n",
    "\n",
    "predictions_lgbm = generate_predictions(ranker, df_, features_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_valid2 = pd.merge(predictions_lgbm.rename(\"lgbm_pred\"),y_test.rename(\"true_labels\"),how=\"inner\",on=\"userId\")['true_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.ranking import mean_average_recall_at_k, mean_average_precision_at_k, mean_average_f1_at_k\n",
    "\n",
    "print(\"mean average recall\",mean_average_recall_at_k(y_true_valid2.values.tolist(),predictions_lgbm.values.tolist()))\n",
    "print(\"mean average precision\",mean_average_precision_at_k(y_true_valid2.values.tolist(),predictions_lgbm.values.tolist()))\n",
    "print(\"mean average f1\",mean_average_f1_at_k(y_true_valid2.values.tolist(),predictions_lgbm.values.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As evident from the results, using a reranker algorithm like LGBMRanker enhances the performance of our recommender system.\n",
    "\n",
    "\n",
    "In summary, we've combined the strengths of both the covisitation recommender, which captures item co-occurrence patterns, and the Word2Vec model, which captures item semantics, to generate an extensive list of candidate items. Then, by using the LGBMRanker algorithm to rerank these candidates, we've achieved better recommendations by considering multiple factors related to item-user interactions and relevance. This approach reflects a well-rounded and effective strategy for improving the recommendation quality of our system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
