{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Deepchecks for Recommender System\n",
    "===============================================================================\n",
    "\n",
    "Deepchecks Recommender  is your go-to tool for developing and evaluating recommender system models, ensuring their robustness before deployment. Our comprehensive testing package not only detects potential failures but also saves you valuable development time. In this quickstart guide, you'll learn how to utilize Deepchecks Recommender to analyze and evaluate various aspects of your recommender system, including data quality, leakage, product associations, cold start detection, and drift. Let's get started.\n",
    "\n",
    "**Step 1: Data Preparation and Auto Analysis**\n",
    "---------------------------------------------\n",
    "\n",
    "To run Deepchecks Recommender, make sure you have the following data for both your training and testing sets:\n",
    "\n",
    "1. User-Item Interaction Data: A structured dataset containing information about user-item interactions. Each record represents a user's interaction with an item, such as viewing, purchasing, or rating.\n",
    "\n",
    "2. Product Information: Additional information about the items in your catalog, like product categories, descriptions, or features.\n",
    "\n",
    "3. User Information (Optional): If available, user-specific data such as demographics, preferences, or historical behavior can enhance the evaluation.\n",
    "\n",
    "4. Your labels : These are not needed for checks that don't require labels (such as the Cold Start Detection check or most data integrity checks), but are needed for many other checks.\n",
    "\n",
    "5. Your model's predictions: These are needed only for the model related checks, shown in the Model Evaluation section of this guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What has been done\n",
    "===============\n",
    "\n",
    "- pass Pylint for all the checks ( docstring, snake_case naming ...)\n",
    "\n",
    "- Speeding up checks (some checks took 50 sec to run like TrainTestOverlap).\n",
    "\n",
    "- fixing the PR (logic, class inheritance, docstring).\n",
    "\n",
    "- First version of the quickstart available to use, which is simple and straightforward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting Up\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Create train and validation split.\n",
    "def split_train_validation(interaction_df : pd.DataFrame,\n",
    "                           session_col : str,\n",
    "                           item_col : str,\n",
    "                           timestamp_col : str, \n",
    "                           test_percentage=0.2,\n",
    "                           random_seed=None):\n",
    "    \n",
    "    assert set([session_col, item_col, timestamp_col]).issubset(interaction_df.columns)\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    interaction_df = interaction_df.sort_values(timestamp_col,ascending=True)\n",
    "    split_index = int(len(interaction_df) * (1 - test_percentage))\n",
    "\n",
    "    train = interaction_df.iloc[:split_index]\n",
    "    test = interaction_df.iloc[split_index:]\n",
    "\n",
    "    # Let's discard overlapping sessions to make train and valid sets disjoints and independent.\n",
    "    overlapping_sessions = set(train[session_col]).intersection(set(test[session_col]))\n",
    "\n",
    "    test = test[~test[session_col].isin(overlapping_sessions)]\n",
    "\n",
    "    data_to_calculate_validation_score = []\n",
    "    new_test = []\n",
    "    for grp in test.groupby(session_col):\n",
    "        cutoff = np.random.randint(1, grp[1].shape[0]) # we want at least a single item in our validation data for each userId\n",
    "        new_test.append(grp[1].iloc[:cutoff])\n",
    "        data_to_calculate_validation_score.append(grp[1].iloc[cutoff:])\n",
    "\n",
    "    test = pd.concat(new_test).reset_index(drop=True)\n",
    "    \n",
    "    test_labels = pd.concat(data_to_calculate_validation_score).reset_index(drop=True)\n",
    "    assert test[timestamp_col].max() < test_labels[timestamp_col].max()\n",
    "    \n",
    "    test_labels = test_labels.groupby(session_col)[item_col].apply(list)\n",
    "    assert (test[session_col].unique() == test_labels.index.values).sum()\n",
    "    \n",
    "    return train,test,test_labels\n",
    "\n",
    "# Create user features.\n",
    "def user_features(X_interaction : pd.DataFrame):\n",
    "    user_df = pd.DataFrame()\n",
    "    user_df['userId'] = X_interaction['userId'].unique()\n",
    "    user_df['mean_rating'] = X_interaction.groupby(\"userId\")['rating'].mean().values\n",
    "    user_df['median_rating'] = X_interaction.groupby(\"userId\")['rating'].median().values\n",
    "    user_df['std_rating'] = X_interaction.groupby(\"userId\")['rating'].std().values\n",
    "    user_df['session_length'] = X_interaction.groupby(\"userId\")['rating'].count().values\n",
    "    return user_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the purpose of this guide, we\\'ll use a small subset of the [movieLens](https://grouplens.org/datasets/movielens/) dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1 µs, sys: 0 ns, total: 1 µs\n",
      "Wall time: 2.86 µs\n"
     ]
    }
   ],
   "source": [
    "%time \n",
    "\n",
    "# Load interaction data.\n",
    "df = pd.read_csv(\"/Users/rayanaay/Downloads/testing_deepchecks/ml-latest-small/ratings.csv\")\n",
    "\n",
    "# Load item data.\n",
    "movie_df =  pd.read_csv(\"/Users/rayanaay/Downloads/testing_deepchecks/ml-latest-small/movies.csv\")\n",
    "\n",
    "# Split interaction data into train and validation data.\n",
    "df['timestamp'] = df['timestamp'].apply(lambda x : datetime.fromtimestamp(x))\n",
    "X_train_interactions, X_test_interactions, y_test = split_train_validation(interaction_df=df,\n",
    "                                           session_col='userId',\n",
    "                                           item_col ='movieId',\n",
    "                                           timestamp_col='timestamp')\n",
    "\n",
    "# Create User Dataframe\n",
    "train_users_df = user_features(X_train_interactions)\n",
    "valid_users_df = user_features(X_test_interactions)\n",
    "\n",
    "# Add targets to the valid users\n",
    "valid_users_df = pd.merge(valid_users_df,y_test.rename('target'),how=\"left\",on=\"userId\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recommender Datasets\n",
    "================================\n",
    "\n",
    "We can now create a Dataset object for the train and test dataframes. This object is\n",
    "used to pass your data to the deepchecks checks.\n",
    "\n",
    "To create a Recommender Dataset, the only required argument is the data\n",
    "itself, but passing only the data will prevent multiple checks from\n",
    "running. In this example we\\'ll define the task type\n",
    "and finally define the\n",
    "metadata columns (the other columns in the dataframe) which we\\'ll use later on in the\n",
    "guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOUT.rst                           \u001b[34mdeepchecks\u001b[m\u001b[m\n",
      "CITATION.cff                        deepchecks_recsys_quickstart2.ipynb\n",
      "CODE_OF_CONDUCT.md                  \u001b[34mdocs\u001b[m\u001b[m\n",
      "CONTRIBUTING.rst                    \u001b[34mexamples\u001b[m\u001b[m\n",
      "DESCRIPTION.rst                     \u001b[34mextensive_testing\u001b[m\u001b[m\n",
      "FAQ.rst                             makefile\n",
      "LICENSE                             \u001b[34mrequirements\u001b[m\u001b[m\n",
      "MANIFEST.in                         setup.py\n",
      "README.md                           spelling-allowlist.txt\n",
      "VERSION                             \u001b[34mtests\u001b[m\u001b[m\n",
      "\u001b[34mbenchmarks\u001b[m\u001b[m                          tox.ini\n",
      "\u001b[34mconda-recipe\u001b[m\u001b[m                        \u001b[34mvenv\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "PackageNotFoundError",
     "evalue": "deepchecks",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPackageNotFoundError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdeepchecks\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mrecommender\u001b[39;00m \u001b[39mimport\u001b[39;00m InteractionDataset, UserDataset,ItemDataset\n\u001b[1;32m      3\u001b[0m \u001b[39m# Interaction Datasets\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39m#################################################################\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_interaction_ds \u001b[39m=\u001b[39m InteractionDataset(df\u001b[39m=\u001b[39mX_train_interactions,                    \n\u001b[1;32m      6\u001b[0m                 features\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mrating\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[1;32m      7\u001b[0m                 datetime_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtimestamp\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      8\u001b[0m                 user_index_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39muserId\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m      9\u001b[0m                 item_index_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmovieId\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Downloads/testing_deepchecks/deepchecks/deepchecks/__init__.py:28\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mimportlib_metadata\u001b[39;00m \u001b[39mimport\u001b[39;00m version\n\u001b[1;32m     27\u001b[0m \u001b[39m# NOTE: it is here, before other import, in order to omit circular import error\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m __version__ \u001b[39m=\u001b[39m version(\u001b[39m'\u001b[39;49m\u001b[39mdeepchecks\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     30\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplotly\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mio\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpio\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/metadata.py:569\u001b[0m, in \u001b[0;36mversion\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mversion\u001b[39m(distribution_name):\n\u001b[1;32m    563\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the version string for the named package.\u001b[39;00m\n\u001b[1;32m    564\u001b[0m \n\u001b[1;32m    565\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package to query.\u001b[39;00m\n\u001b[1;32m    566\u001b[0m \u001b[39m    :return: The version string for the package as defined in the package's\u001b[39;00m\n\u001b[1;32m    567\u001b[0m \u001b[39m        \"Version\" metadata key.\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 569\u001b[0m     \u001b[39mreturn\u001b[39;00m distribution(distribution_name)\u001b[39m.\u001b[39mversion\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/metadata.py:542\u001b[0m, in \u001b[0;36mdistribution\u001b[0;34m(distribution_name)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdistribution\u001b[39m(distribution_name):\n\u001b[1;32m    537\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the ``Distribution`` instance for the named package.\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \n\u001b[1;32m    539\u001b[0m \u001b[39m    :param distribution_name: The name of the distribution package as a string.\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[39m    :return: A ``Distribution`` instance (or subclass thereof).\u001b[39;00m\n\u001b[1;32m    541\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m Distribution\u001b[39m.\u001b[39;49mfrom_name(distribution_name)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.9/3.9.17_1/Frameworks/Python.framework/Versions/3.9/lib/python3.9/importlib/metadata.py:196\u001b[0m, in \u001b[0;36mDistribution.from_name\u001b[0;34m(cls, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[39mreturn\u001b[39;00m dist\n\u001b[1;32m    195\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m PackageNotFoundError(name)\n",
      "\u001b[0;31mPackageNotFoundError\u001b[0m: deepchecks"
     ]
    }
   ],
   "source": [
    "from deepchecks.recommender import InteractionDataset, UserDataset,ItemDataset\n",
    "\n",
    "# Interaction Datasets\n",
    "#################################################################\n",
    "train_interaction_ds = InteractionDataset(df=X_train_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "valid_interaction_ds = InteractionDataset(df=X_test_interactions,                    \n",
    "                features=['rating'],\n",
    "                datetime_name='timestamp',\n",
    "                user_index_name='userId',\n",
    "                item_index_name='movieId')\n",
    "\n",
    "# User Datasets\n",
    "#################################################################\n",
    "train_user_ds = UserDataset(df = train_users_df,\n",
    "                label = None,                    \n",
    "                features=['mean_rating', 'session_length'],\n",
    "                cat_features=None)\n",
    "\n",
    "valid_user_ds = UserDataset(df = valid_users_df,\n",
    "                label = \"target\",                    \n",
    "                features=['mean_rating', 'session_length'],\n",
    "                cat_features=None)\n",
    "            \n",
    "# Item Dataset\n",
    "#################################################################\n",
    "item_ds = ItemDataset(df=movie_df,\n",
    "                      item_column_name='title',\n",
    "                      features=['title','genres'],\n",
    "                      cat_features=['title','genres'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Recommender Model Class\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoOccurrenceRecommender:\n",
    "    def __init__(self, col, item_col, num_predictions=20):\n",
    "        self.col = col\n",
    "        self.item_col = item_col\n",
    "        self.num_predictions = num_predictions\n",
    "        self.co_occurences = defaultdict(Counter)\n",
    "        \n",
    "    def fit(self, X_train):\n",
    "        # Make a copy of the training data\n",
    "        filtered_interactions = X_train.copy()\n",
    "        \n",
    "        # Create a new column with the previous item in each user session\n",
    "        prev_item_col = f'prev_{self.item_col}'\n",
    "        filtered_interactions[prev_item_col] = filtered_interactions.groupby(self.col)[self.item_col].shift(1).astype(\"Int64\").dropna()\n",
    "        \n",
    "        # Create a DataFrame with columns 'previous item' and 'item'\n",
    "        products_association_df = filtered_interactions[[prev_item_col, self.item_col]].copy().dropna()\n",
    "        \n",
    "        # Generate associations between 'previous item' and 'item'\n",
    "        for row in products_association_df.itertuples(index=False):\n",
    "            self.co_occurences[row[0]][row[1]] += 1\n",
    "\n",
    "    def predict(self, X_valid):\n",
    "        # Generate predictions for the validation set\n",
    "        labels = []\n",
    "        X_test_session_items = X_valid.groupby(self.col)[self.item_col].apply(list)\n",
    "\n",
    "        for items in X_test_session_items:\n",
    "            items = list(dict.fromkeys(items[::-1]))\n",
    "\n",
    "            counter = Counter()\n",
    "\n",
    "            for item in items:\n",
    "                subsequent_item_counter = self.co_occurences.get(item)\n",
    "                if subsequent_item_counter:\n",
    "                    counter += subsequent_item_counter\n",
    "            \n",
    "            # Get the top N recommended items based on the associations\n",
    "            recommendations = [item for item, cnt in counter.most_common(self.num_predictions) if item not in items]\n",
    "            labels.append(recommendations)\n",
    "        \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "# Assuming you have X_train and X_valid DataFrames with columns 'userId' and 'movieId'\n",
    "# Create an instance of the AssociationRecommender class\n",
    "recommender = CoOccurrenceRecommender(col='userId',\n",
    "                                      item_col='movieId',\n",
    "                                      num_predictions=20)\n",
    "\n",
    "# Fit the model on the training data\n",
    "recommender.fit(X_train_interactions)\n",
    "\n",
    "# Generate predictions for the validation data\n",
    "predictions = recommender.predict(X_test_interactions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.ranking import mean_average_recall_at_k,mean_average_precision_at_k\n",
    "\n",
    "mean_average_recall_at_k(y_test.values.tolist(),\n",
    "                             predictions,\n",
    "                             k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time\n",
    "\n",
    "from deepchecks.recommender.checks import SamplePerformance\n",
    "\n",
    "check = SamplePerformance(scorers=['mean_average_precision_at_k',\n",
    "                                   'mean_average_recall_at_k',\n",
    "                                   'mean_reciprocal_rank'])\n",
    "\n",
    "result = check.run(valid_user_ds,\n",
    "                   y_pred=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DateTrainTestLeakageOverlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import DateTrainTestLeakageOverlap\n",
    "\n",
    "check = DateTrainTestLeakageOverlap(validation_per_user=False)\n",
    "\n",
    "result = check.run(train_dataset=train_interaction_ds,\n",
    "                   test_dataset=valid_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cold Start Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks  import ColdStartDetection\n",
    "\n",
    "all_interaction_ds = train_interaction_ds + valid_interaction_ds\n",
    "check = ColdStartDetection()\n",
    "result = check.run(all_interaction_ds)\n",
    "result.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product Association\n",
    "====================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import ProductAssociation\n",
    "\n",
    "check = ProductAssociation(max_timestamp_delta=3600)\n",
    "result = check.run(all_interaction_ds,\n",
    "                   item_dataset=item_ds\n",
    "                   )\n",
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "User Session Length Drift\n",
    "===================================\n",
    "\n",
    "Also in the \\\"Didn\\'t Pass\\\" tab we can see the two segment performance\n",
    "checks - Property Segment Performance and Metadata Segment Performance.\n",
    "These use the metadata columns  of user related information OR our\n",
    "calculated properties to try and **automatically** detect significant data\n",
    "segments on which our model performs badly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import UserSessionDrift\n",
    "\n",
    "check = UserSessionDrift()\n",
    "\n",
    "result = check.run(train_dataset = train_interaction_ds,\n",
    "                   test_dataset = valid_interaction_ds)\n",
    "result.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Prediction Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import PredictionPopularityDrift\n",
    "\n",
    "check = PredictionPopularityDrift()\n",
    "result = check.run(valid_user_ds,\n",
    "                   y_pred=predictions,\n",
    "                   interaction_dataset=train_interaction_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Label Popularity Drift\n",
    "================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import LabelPopularityDrift\n",
    "\n",
    "check = LabelPopularityDrift()\n",
    "result = check.run(valid_user_ds,\n",
    "                   interaction_dataset=train_interaction_ds+valid_interaction_ds)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepchecks.recommender.checks import SegmentPerformance\n",
    "import traceback\n",
    "try:\n",
    "    result = SegmentPerformance(feature_1='session_length',\n",
    "                       feature_2='mean_rating',\n",
    "                       alternative_scorer={'recall':'mean_average_precision_at_k'},\n",
    "                       max_segments=3\n",
    "                       ).run(valid_user_ds, y_pred=predictions)\n",
    "except:\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's left\n",
    "===============\n",
    "- **merging the PR.**\n",
    "\n",
    "- **finalizing the quickstart**\n",
    "    - description (markdown).\n",
    "    - Initial simplified description for the heuristic model used.\n",
    "    - replace SegmentPerformance by WeakSegmentPerformance because the first is deprecated.\n",
    "    - push it without the display.\n",
    "- **make pylint (specific to deepchecks)**\n",
    "\n",
    "- **Suites & Conditions**\n",
    "\n",
    "- **adding a Reranker to the quickstart**\n",
    "    - lightgbm classifier.\n",
    "    - use of classic checks of classifier.\n",
    "- **try all others scorers to solve potential issues**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
